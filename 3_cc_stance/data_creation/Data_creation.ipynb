{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import Counter,defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from numpy.random import RandomState\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANCES = [\"agree\", \"neutral\", \"disagree\"]\n",
    "CLASS_NUMS = {s: i for i, s in enumerate(STANCES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     1,
     5,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# move to utils.py later\n",
    "nli2stance = {'entailment': CLASS_NUMS['agree'], \n",
    "              'neutral': CLASS_NUMS['neutral'], \n",
    "              'contradiction': CLASS_NUMS['disagree']}\n",
    "\n",
    "float2stance = {1.0: CLASS_NUMS['agree'],\n",
    "               0.0: CLASS_NUMS['neutral'],\n",
    "               -1.0: CLASS_NUMS['disagree']}\n",
    "\n",
    "stance2nli = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "                \n",
    "def stance_reg(label):\n",
    "    \"\"\"\n",
    "    Regularize the stance labels \n",
    "    :param label: a label of str (agree(s)/entailment, neutral, disagree(s)/contradiction), \n",
    "     int (0, 1, 2) or str of int, or float (1.0, 0.0, -1.0)\n",
    "    :return: the label as the corresponding class_num\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(label) == str:\n",
    "        if label.isalpha(): # could be a,n,d or NLI labels\n",
    "            if label in STANCES:\n",
    "                return CLASS_NUMS[label]\n",
    "            elif label[-1] == 's':\n",
    "                return CLASS_NUMS[label[:-1]]\n",
    "            else:\n",
    "                return nli2stance[label]\n",
    "        else: # label is str of (0, 1, 2)\n",
    "            return int(label)\n",
    "    elif type(label) == float:\n",
    "        return float2stance[label]\n",
    "    else:\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_backtrans_train(train_df,language,upsample=False):\n",
    "    \"\"\"\n",
    "    Create df with backtranslations of train_df \n",
    "    :param train_df: base training data\n",
    "    :param language: 'fr' or 'zh'\n",
    "    :return: new df with previous training data + augmented data\n",
    "    \"\"\"\n",
    "    \n",
    "    backtrans_df = pd.DataFrame({\n",
    "        'round':train_df['round'].values,\n",
    "        'batch':train_df.batch.values,\n",
    "        'sent_id':train_df.sent_id.values,\n",
    "        'stance':train_df.stance.values,\n",
    "        'sentence':[get_backtrans(guid,language) for guid in train_df.guid],\n",
    "        'is_high_iaa':train_df.is_high_iaa.values,\n",
    "        'guid':[guid+'_'+language for guid in train_df.guid]\n",
    "    })\n",
    "    \n",
    "    backtrans_df = backtrans_df.loc[backtrans_df.stance.isin({'disagrees','disagree'})].append(\n",
    "        train_df,ignore_index=True)\n",
    "    return backtrans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# labeled_data = pd.read_pickle('./data/labeled_data_df.pkl')\n",
    "# labeled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# labeled_data.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Estimated labels (MTurk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2050, 8), (2042, 8))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_labels = pd.read_csv('/Users/yiweiluo/scientific-debates/\\\n",
    "3_cc_stance/MTurk/MTurk_results/sent_scores_df.tsv',delimiter='\\t',index_col=0)\n",
    "est_labels['max_prob_label'] = est_labels[['disagree','neutral','agree']].idxmax(axis=1)\n",
    "dedup_est_labels = est_labels.drop_duplicates('sentence',keep='first')\n",
    "est_labels.shape, dedup_est_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "assert len(dedup_est_labels) == 2042\n",
    "dedup_est_labels['guid'] = [\"{}_{}_{}\".format(row['round'],row['batch'],row['sent_id']) \n",
    "                      for _,row in dedup_est_labels.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>batch</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>disagree</th>\n",
       "      <th>neutral</th>\n",
       "      <th>agree</th>\n",
       "      <th>sentence</th>\n",
       "      <th>max_prob_label</th>\n",
       "      <th>guid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t0</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.260963</td>\n",
       "      <td>0.734797</td>\n",
       "      <td>Warmer-than-normal sea surface temperatures are a key player in the development of hurricanes such as Katrina and superstorm Sandy, which hit the U.S. east coast in 2011.</td>\n",
       "      <td>agree</td>\n",
       "      <td>1_0_t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t1</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.996214</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>We will continue to rely in part on fossil fuels while we transition to a low-carbon economy .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1_0_t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t10</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.996503</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>The actual rise in sea levels measured only 1.2 millimeters instead of the previously accepted 1.6 to 1.9 millimeters.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1_0_t10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t11</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>Claims of global warming have been greatly exaggerated.</td>\n",
       "      <td>disagree</td>\n",
       "      <td>1_0_t11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t12</td>\n",
       "      <td>0.035201</td>\n",
       "      <td>0.959757</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>The Intergovernmental Panel on Climate Change should be clearer on how it draws conclusions from the body of research it assesses when gauging the impacts of global warming.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1_0_t12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round  batch sent_id  disagree   neutral     agree  \\\n",
       "0  1      0      t0      0.004241  0.260963  0.734797   \n",
       "1  1      0      t1      0.001548  0.996214  0.002239   \n",
       "2  1      0      t10     0.001440  0.996503  0.002057   \n",
       "3  1      0      t11     0.996815  0.001588  0.001596   \n",
       "4  1      0      t12     0.035201  0.959757  0.005042   \n",
       "\n",
       "                                                                                                                                                                        sentence  \\\n",
       "0  Warmer-than-normal sea surface temperatures are a key player in the development of hurricanes such as Katrina and superstorm Sandy, which hit the U.S. east coast in 2011.      \n",
       "1  We will continue to rely in part on fossil fuels while we transition to a low-carbon economy .                                                                                  \n",
       "2  The actual rise in sea levels measured only 1.2 millimeters instead of the previously accepted 1.6 to 1.9 millimeters.                                                          \n",
       "3  Claims of global warming have been greatly exaggerated.                                                                                                                         \n",
       "4  The Intergovernmental Panel on Climate Change should be clearer on how it draws conclusions from the body of research it assesses when gauging the impacts of global warming.   \n",
       "\n",
       "  max_prob_label     guid  \n",
       "0  agree          1_0_t0   \n",
       "1  neutral        1_0_t1   \n",
       "2  neutral        1_0_t10  \n",
       "3  disagree       1_0_t11  \n",
       "4  neutral        1_0_t12  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedup_est_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Raw labels (MTurk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "worker_labels_per_round = pickle.load(open('../MTurk/MTurk_results/full_ratings_per_round.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PROP_AGREE = 0.75\n",
    "NUM_ROUNDS, NUM_BATCHES, NUM_WORKERS = 5, 10, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "code_folding": [
     1,
     22
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2042, 7)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_df,batch_df,sentid_df,stance_df,text_df,high_iaa_df,guid_df = [],[],[],[],[],[],[]\n",
    "for r in range(1,1+NUM_ROUNDS):\n",
    "    for b in range(NUM_BATCHES):\n",
    "        labels = worker_labels_per_round[r][b]\n",
    "        for s_id in labels.index[5:-1]:\n",
    "            round_df.append(r)\n",
    "            batch_df.append(b)\n",
    "            sentid_df.append(s_id)\n",
    "            text_df.append(labels.loc[s_id].sentence)\n",
    "            guid_df.append(\"{}_{}_{}\".format(r,b,s_id))\n",
    "            \n",
    "            ratings = labels.loc[s_id][['worker_{}'.format(w_id) for w_id in range(NUM_WORKERS)]].values\n",
    "            top_rating = Counter(ratings).most_common()[0]\n",
    "            if top_rating[-1] >= PROP_AGREE*NUM_WORKERS:\n",
    "                stance_df.append(top_rating[0])\n",
    "                high_iaa_df.append(True)\n",
    "            else:\n",
    "                stance_df.append(est_labels.loc[(est_labels['round'] == r) & \n",
    "                                             (est_labels['batch'] == b) & \n",
    "                                             (est_labels['sent_id'] == s_id)].max_prob_label.values[0])\n",
    "                high_iaa_df.append(False)\n",
    "\n",
    "mturk_df = pd.DataFrame({'round':round_df,\"batch\":batch_df,\"sent_id\":sentid_df,\"stance\":stance_df,\n",
    "                 \"sentence\":text_df,'is_high_iaa':high_iaa_df,'guid':guid_df})\n",
    "mturk_df = mturk_df.drop_duplicates('sentence',keep='first')\n",
    "mturk_df.reset_index(drop=True,inplace=True)\n",
    "mturk_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>batch</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>stance</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_high_iaa</th>\n",
       "      <th>guid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t0</td>\n",
       "      <td>agree</td>\n",
       "      <td>Warmer-than-normal sea surface temperatures are a key player in the development of hurricanes such as Katrina and superstorm Sandy, which hit the U.S. east coast in 2011.</td>\n",
       "      <td>False</td>\n",
       "      <td>1_0_t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>We will continue to rely in part on fossil fuels while we transition to a low-carbon economy .</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t10</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The actual rise in sea levels measured only 1.2 millimeters instead of the previously accepted 1.6 to 1.9 millimeters.</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t11</td>\n",
       "      <td>disagrees</td>\n",
       "      <td>Claims of global warming have been greatly exaggerated.</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t12</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The Intergovernmental Panel on Climate Change should be clearer on how it draws conclusions from the body of research it assesses when gauging the impacts of global warming.</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round  batch sent_id     stance  \\\n",
       "0  1      0      t0      agree       \n",
       "1  1      0      t1      neutral     \n",
       "2  1      0      t10     neutral     \n",
       "3  1      0      t11     disagrees   \n",
       "4  1      0      t12     neutral     \n",
       "\n",
       "                                                                                                                                                                        sentence  \\\n",
       "0  Warmer-than-normal sea surface temperatures are a key player in the development of hurricanes such as Katrina and superstorm Sandy, which hit the U.S. east coast in 2011.      \n",
       "1  We will continue to rely in part on fossil fuels while we transition to a low-carbon economy .                                                                                  \n",
       "2  The actual rise in sea levels measured only 1.2 millimeters instead of the previously accepted 1.6 to 1.9 millimeters.                                                          \n",
       "3  Claims of global warming have been greatly exaggerated.                                                                                                                         \n",
       "4  The Intergovernmental Panel on Climate Change should be clearer on how it draws conclusions from the body of research it assesses when gauging the impacts of global warming.   \n",
       "\n",
       "   is_high_iaa     guid  \n",
       "0  False        1_0_t0   \n",
       "1  True         1_0_t1   \n",
       "2  True         1_0_t10  \n",
       "3  True         1_0_t11  \n",
       "4  True         1_0_t12  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mturk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1119\n",
       "False    923 \n",
       "Name: is_high_iaa, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mturk_df.is_high_iaa.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Back translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "back_trans_fr = pd.read_csv('../datasets/mturk_french_backtranslations.tsv',sep='\\t',\n",
    "                        header=0,index_col=0)\n",
    "back_trans_zh = pd.read_csv('../datasets/mturk_zh_backtranslations.tsv',sep='\\t',\n",
    "                        header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_backtrans(guid,language):\n",
    "    r,b,s_id = guid.split('_')\n",
    "    if language == 'fr':\n",
    "        return back_trans_fr.loc[(back_trans_fr['round'] == int(r)) &\n",
    "                                (back_trans_fr['batch'] == int(b)) &\n",
    "                                (back_trans_fr['sent_id'] == s_id)].backtranslation.values[0]\n",
    "    else:\n",
    "        return back_trans_zh.loc[(back_trans_fr['round'] == int(r)) &\n",
    "                                (back_trans_fr['batch'] == int(b)) &\n",
    "                                (back_trans_fr['sent_id'] == s_id)].backtranslation_zh_en.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Warmer than normal sea surface temperatures are a key player in the development of hurricanes such as Katrina and Sandstorm Sandy, which hit the east coast of the United States in 2011.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_backtrans('1_0_t0','fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Above-normal sea-level temperatures were a key factor in the development of hurricanes such as Hurricane Katrina and Sandy, which hit the US East Coast in 2011.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_backtrans('1_0_t0','zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sentence windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fnames = os.listdir('../../1_data_scraping/fulltexts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_round_data = {r: {} for r in range(1,6)}\n",
    "for round_no in range(1,6):\n",
    "    all_round_data[round_no] = pickle.load(open('/Users/yiweiluo/Dropbox/research/QP2/code/Fox_and_friends/\\\n",
    "LIVE_ROUND{}_BATCH_DATA.pkl'.format(round_no),'rb'))\n",
    "    \n",
    "data_for_mturk_df = pd.read_pickle('/Users/yiweiluo/Dropbox/research/QP2/code/Fox_and_friends/\\\n",
    "data_for_mturk_2020.pkl')\n",
    "data_for_mturk_df_old = pd.read_pickle('/Users/yiweiluo/Dropbox/research/QP2/code/Fox_and_friends/\\\n",
    "data_for_mturk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def get_window(guid,window_size):\n",
    "    r,b,s_id = guid.split('_')\n",
    "    target_sent = mturk_df.loc[(mturk_df['round'] == int(r)) &\n",
    "                                (mturk_df['batch'] == int(b)) &\n",
    "                                (mturk_df['sent_id'] == s_id)].sentence.values[0]\n",
    "\n",
    "#     print('Target sent:',target_sent)\n",
    "#     print('Round: {}, batch: {}, sent_id: {}'.format(r,b,s_id))\n",
    "    rb_df = pd.DataFrame(all_round_data[int(r)][int(b)])\n",
    "    df_key = rb_df.loc[rb_df.sent_id == s_id].df_key.values[0]\n",
    "#     print('df key:',df_key)\n",
    "    \n",
    "    if int(r) < 5:\n",
    "        sent_key = data_for_mturk_df_old.loc[df_key].sent_key\n",
    "    else:\n",
    "        sent_key = data_for_mturk_df.loc[df_key].sent_key\n",
    "        \n",
    "    url = sent_key.split(' of ')[-1].split('://')[-1]\n",
    "    #print('url:',url)\n",
    "    \n",
    "    fname = url.replace('/','[SEP]')\n",
    "    fname = '{}.txt'.format(fname) if '{}.txt'.format(fname) in fnames else '{}.txt'.format(fname[:90])\n",
    "    #print('fname:',fname)\n",
    "    \n",
    "    if fname in fnames:\n",
    "        with open(os.path.join('../../1_data_scraping/fulltexts',fname)) as f:\n",
    "            text = f.readlines()\n",
    "        if len(text) > 0:\n",
    "            text = text[0]\n",
    "\n",
    "            text_sents = sent_tokenize(text)\n",
    "            sent_with_target = process.extract(target_sent, text_sents, limit=1)\n",
    "            #print('Found sentence containing target sent:',sent_with_target)\n",
    "            ix_target_sent = text_sents.index(sent_with_target[0][0])\n",
    "\n",
    "            w_start = max(0,ix_target_sent-window_size)\n",
    "            w_end = min(ix_target_sent+window_size,len(text_sents)-1)\n",
    "            w_left = text_sents[w_start:ix_target_sent]\n",
    "            w_right = text_sents[ix_target_sent+1:w_end+1]\n",
    "            #print('Left sentence(s):',w_left)\n",
    "            #print('Right sentence(s):',w_right)\n",
    "            BERT_input = '[SEP] '.join(w_left)+' [SEP] [CLS] '+target_sent+' [SEP] '+' [SEP] '.join(w_right)\n",
    "            if BERT_input[:6] != ' [SEP]':\n",
    "                #print('Padding beginning with [SEP]...')\n",
    "                BERT_input = '[SEP] '+BERT_input\n",
    "                \n",
    "            return BERT_input\n",
    "        else:\n",
    "            print('Fulltext is empty!')\n",
    "    else:\n",
    "        print('Fulltext file not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP] “I think it’s very important to remind people the scope of what can happen with the hurricane season.”  Nonetheless, the events surrounding the hurricane, which caused $108 billion of damage, continue to interest to the scientific community. [SEP] [CLS] Warmer-than-normal sea surface temperatures are a key player in the development of hurricanes such as Katrina and superstorm Sandy, which hit the U.S. east coast in 2011. [SEP] “These storms may not have been caused by global warming, but because the ocean’s surface is warmer, it makes the storm more powerful,” Thomas Wagner, cryosphere program manager at NASA headquarters in Washington, D.C. told FoxNews.com.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_window('1_0_t0',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP] “We haven’t had a Category 3 hit the U.S. in 10 years – I think there’s a lot of complacency out there,” she said during a panel discussion at an American Meteorological Society conference in June.[SEP] “I think it’s very important to remind people the scope of what can happen with the hurricane season.”  Nonetheless, the events surrounding the hurricane, which caused $108 billion of damage, continue to interest to the scientific community. [SEP] [CLS] Warmer-than-normal sea surface temperatures are a key player in the development of hurricanes such as Katrina and superstorm Sandy, which hit the U.S. east coast in 2011. [SEP] “These storms may not have been caused by global warming, but because the ocean’s surface is warmer, it makes the storm more powerful,” Thomas Wagner, cryosphere program manager at NASA headquarters in Washington, D.C. told FoxNews.com. [SEP] “Then, because sea level is higher, the water can go further inland from the storm surge.”  President Obama briefly addressed the issue of climate change during a speech in New Orleans Thursday.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_window('1_0_t0',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SemEval tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 395)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semeval_test = pd.read_csv('../datasets/StanceDataset/test.csv',header=0,encoding='utf-8',engine='python')\n",
    "semeval_test = semeval_test[semeval_test['Target'] == 'Climate Change is a Real Concern']\n",
    "\n",
    "semeval_train = pd.read_csv('../datasets/StanceDataset/train.csv',header=0,encoding='utf-8', engine='python')\n",
    "semeval_train = semeval_train[semeval_train['Target'] == 'Climate Change is a Real Concern']\n",
    "\n",
    "len(semeval_test),len(semeval_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "semeval_test = semeval_test[['Tweet','Stance']]\n",
    "semeval_train = semeval_train[['Tweet','Stance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweetstance2label = {'NONE': CLASS_NUMS['neutral'],\n",
    "                    'FAVOR': CLASS_NUMS['agree'],\n",
    "                    'AGAINST': CLASS_NUMS['disagree']}\n",
    "\n",
    "semeval_test['stance'] = semeval_test['Stance'].apply(lambda x: tweetstance2label[x])\n",
    "semeval_train['stance'] = semeval_train['Stance'].apply(lambda x: tweetstance2label[x])\n",
    "semeval_df = semeval_test.append(semeval_train,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    335\n",
       "1    203\n",
       "2    26 \n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semeval_df.stance.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Add additional info: original source media leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_orig_media_slant(guid):\n",
    "    r,b,s_id = guid.split('_')\n",
    "    if int(r) < 5:\n",
    "        df_ = data_for_mturk_df_old\n",
    "    else:\n",
    "        df_ = data_for_mturk_df\n",
    "    \n",
    "    b_df_ = pd.DataFrame(all_round_data[int(r)][int(b)])\n",
    "    df_key = b_df_.loc[b_df_.sent_id == s_id].df_key.values[0]\n",
    "    \n",
    "    def str_to_int(s):\n",
    "        return int(s == 'pro') # 1 for pro, 0 for anti\n",
    "        \n",
    "    return str_to_int(df_.iloc[df_key].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create train/dev/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "\n",
      "[SEP] “I think it’s very important to remind people the scope of what can happen with the hurricane season.”  Nonetheless, the events surrounding the hurricane, which caused $108 billion of damage, continue to interest to the scientific community. [SEP] [CLS] Warmer-than-normal sea surface temperatures are a key player in the development of hurricanes such as Katrina and superstorm Sandy, which hit the U.S. east coast in 2011. [SEP] “These storms may not have been caused by global warming, but because the ocean’s surface is warmer, it makes the storm more powerful,” Thomas Wagner, cryosphere program manager at NASA headquarters in Washington, D.C. told FoxNews.com.\n",
      "\n",
      "\n",
      "How the Intergovernmental Panel on Climate Change should draw clearer conclusions from the research findings it assesses when assessing the effects of global warming.\n"
     ]
    }
   ],
   "source": [
    "df_getter = {'raw_mturk': mturk_df,\n",
    "            'est_mturk': dedup_est_labels,\n",
    "            'semeval': semeval_df}\n",
    "\n",
    "print(get_orig_media_slant('1_0_t12'))\n",
    "print('\\n')\n",
    "print(get_window('1_0_t0',1))\n",
    "print('\\n')\n",
    "print(get_backtrans('1_0_t12','zh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>batch</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>stance</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_high_iaa</th>\n",
       "      <th>guid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t12</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The Intergovernmental Panel on Climate Change should be clearer on how it draws conclusions from the body of research it assesses when gauging the impacts of global warming.</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round  batch sent_id   stance  \\\n",
       "4  1      0      t12     neutral   \n",
       "\n",
       "                                                                                                                                                                        sentence  \\\n",
       "4  The Intergovernmental Panel on Climate Change should be clearer on how it draws conclusions from the body of research it assesses when gauging the impacts of global warming.   \n",
       "\n",
       "   is_high_iaa     guid  \n",
       "4  True         1_0_t12  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_getter['raw_mturk'].loc[df_getter['raw_mturk'].guid == '1_0_t12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def write_data(name,seed,desc,train_df,test_df,dev_df=None,do_downsample=False):\n",
    "    \"\"\"\n",
    "    Writes data to a directory containing train.tsv, test.tsv, and optionally dev.tsv.\n",
    "    :param name: name of directory (type of train/eval data)\n",
    "    :param seed: random_seed used\n",
    "    :param desc: list of type str with manipulations made (e.g., downsampled, upsampled, backtrans_fr, window_1)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check that train and eval text are deduplicated\n",
    "    train_guids = set([x.replace('_fr','').replace('_zh','') for x in train_df.guid])\n",
    "    test_guids = set([x.replace('_fr','').replace('_zh','') for x in test_df.guid])\n",
    "    assert train_guids.intersection(test_guids) == set()\n",
    "    print(\"Train/test text overlap:\",set(train_df.sentence).intersection(set(test_df.sentence)))\n",
    "    if dev_df is not None:\n",
    "        dev_guids = set([x.replace('_fr','').replace('_zh','') for x in dev_df.guid])\n",
    "        assert train_guids.intersection(dev_guids) == set()\n",
    "        print(\"Train/dev text overlap:\",set(train_df.sentence).intersection(set(dev_df.sentence)))\n",
    "    \n",
    "    # Make save_dir\n",
    "    if do_downsample:\n",
    "        desc.append('downsampled')\n",
    "    save_dir = os.path.join('save',\"_\".join([name]+desc+[str(seed)]))\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "        \n",
    "    # Regularize labels\n",
    "    train_df['reg_stance'] = train_df['stance'].apply(stance_reg)\n",
    "    test_df['reg_stance'] = test_df['stance'].apply(stance_reg)\n",
    "    if dev_df is not None:\n",
    "        dev_df['reg_stance'] = dev_df['stance'].apply(stance_reg) \n",
    "        \n",
    "    # Aggregate examples by stance for downsampling/upsampling needs\n",
    "    train_df_by_stance = {s: train_df.loc[train_df.reg_stance == i] for i,s in enumerate(STANCES)} \n",
    "    test_df_by_stance = {s: test_df.loc[test_df.reg_stance == i] for i,s in enumerate(STANCES)}\n",
    "    dev_df_by_stance = {s: dev_df.loc[dev_df.reg_stance == i] for i,s in enumerate(STANCES)} if dev_df is not None else None\n",
    "\n",
    "    # Split X, Y\n",
    "    train_X_by_stance = {s: train_df_by_stance[s].sentence.values for s in STANCES}\n",
    "    test_X_by_stance = {s: test_df_by_stance[s].sentence.values for s in STANCES}\n",
    "    dev_X_by_stance = {s: dev_df_by_stance[s].sentence.values for s in STANCES} if dev_df is not None else None\n",
    "    \n",
    "    train_Y_by_stance = {s: train_df_by_stance[s].reg_stance.values for s in STANCES} \n",
    "    dev_Y_by_stance = {s: dev_df_by_stance[s].reg_stance.values for s in STANCES} if dev_df is not None else None\n",
    "    test_Y_by_stance = {s: test_df_by_stance[s].reg_stance.values for s in STANCES}\n",
    "\n",
    "    train_nli_by_stance = {s: train_df_by_stance[s].reg_stance.apply(lambda x: stance2nli[x]).values for s in STANCES}\n",
    "    dev_nli_by_stance = {s: dev_df_by_stance[s].reg_stance.apply(lambda x: stance2nli[x]).values for s in STANCES} if dev_df is not None else None\n",
    "    test_nli_by_stance = {s: test_df_by_stance[s].reg_stance.apply(lambda x: stance2nli[x]).values for s in STANCES}\n",
    "\n",
    "    if do_downsample:\n",
    "        min_N = min([len(train_X_by_stance[s]) for s in STANCES])\n",
    "        print('Downsampling to ~{} examples per stance.'.format(min_N))\n",
    "        for s in STANCES:\n",
    "            train_X_by_stance[s] = train_X_by_stance[s][:min_N+50]\n",
    "\n",
    "    trX = []\n",
    "    trB = []\n",
    "    trY = []\n",
    "    trNLI = []\n",
    "    for i,s in enumerate(STANCES):\n",
    "        for t, y, nli in zip(train_X_by_stance[s], train_Y_by_stance[s], train_nli_by_stance[s]):\n",
    "            #for text_b in TEXT_BS:\n",
    "            trX.append(t)\n",
    "            #trB.append(text_b)\n",
    "            trY.append(y)\n",
    "            trNLI.append(nli)\n",
    "\n",
    "    teX = []\n",
    "    teB = []\n",
    "    teY = []\n",
    "    teNLI = []\n",
    "    for i,s in enumerate(STANCES):\n",
    "        for t, y, nli in zip(test_X_by_stance[s], test_Y_by_stance[s], test_nli_by_stance[s]):\n",
    "            #for text_b in TEXT_BS:\n",
    "            teX.append(t)\n",
    "            #teB.append(text_b)\n",
    "            teY.append(y)\n",
    "            teNLI.append(nli)\n",
    "\n",
    "    if dev_df is not None:\n",
    "        vaX = []\n",
    "        vaY = []\n",
    "        vaNLI = []\n",
    "        for i,s in enumerate(STANCES):\n",
    "            for t, y, nli in zip(dev_X_by_stance[s], dev_Y_by_stance[s], dev_nli_by_stance[s]):\n",
    "                vaX.append(t)\n",
    "                vaY.append(y)\n",
    "                vaNLI.append(nli)\n",
    "\n",
    "\n",
    "    test_dat = pd.DataFrame({'sentence':teX,'stance':teY,'nli_label':teNLI})\n",
    "    train_dat = pd.DataFrame({'sentence':trX,'stance':trY,'nli_label':trNLI}) \n",
    "    val_dat = pd.DataFrame({'sentence':vaX,'stance':vaY,'nli_label':vaNLI}) if dev_df is not None else None\n",
    "    \n",
    "    print('Train distribution:')\n",
    "    print(train_dat.stance.value_counts()) \n",
    "    print(train_dat.nli_label.value_counts())\n",
    "    if dev_df is not None:\n",
    "        print('\\nDev distribution:')\n",
    "        print(val_dat.stance.value_counts())\n",
    "        print(val_dat.nli_label.value_counts())\n",
    "    print('\\nTest distribution:')\n",
    "    print(test_dat.stance.value_counts())\n",
    "    print(test_dat.stance.value_counts()/np.sum(test_dat.stance.value_counts().values))\n",
    "    print(test_dat.nli_label.value_counts())\n",
    "\n",
    "    train_dat.to_csv(save_dir+'/train.tsv',sep='\\t',header=None,index=False)\n",
    "    if dev_df is not None:\n",
    "        val_dat.to_csv(save_dir+'/dev.tsv',sep='\\t',header=None,index=False)\n",
    "    test_dat.to_csv(save_dir+'/test.tsv',sep='\\t',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Train on high_iaa, test on rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1119, 923)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guids_train = mturk_df.loc[mturk_df.is_high_iaa].guid.values\n",
    "guids_test = mturk_df.loc[~mturk_df.is_high_iaa].guid.values\n",
    "len(guids_train),len(guids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1119, 7), (923, 7))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = mturk_df.loc[mturk_df.guid.isin(guids_train)]\n",
    "test_df = mturk_df.loc[mturk_df.guid.isin(guids_test)]\n",
    "train_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>batch</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>stance</th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_high_iaa</th>\n",
       "      <th>guid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>We will continue to rely in part on fossil fuels while we transition to a low-carbon economy .</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t10</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The actual rise in sea levels measured only 1.2 millimeters instead of the previously accepted 1.6 to 1.9 millimeters.</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t11</td>\n",
       "      <td>disagrees</td>\n",
       "      <td>Claims of global warming have been greatly exaggerated.</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t12</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The Intergovernmental Panel on Climate Change should be clearer on how it draws conclusions from the body of research it assesses when gauging the impacts of global warming.</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t15</td>\n",
       "      <td>agrees</td>\n",
       "      <td>Simply reducing emissions will not sufficiently limit global warming.</td>\n",
       "      <td>True</td>\n",
       "      <td>1_0_t15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   round  batch sent_id     stance  \\\n",
       "1  1      0      t1      neutral     \n",
       "2  1      0      t10     neutral     \n",
       "3  1      0      t11     disagrees   \n",
       "4  1      0      t12     neutral     \n",
       "7  1      0      t15     agrees      \n",
       "\n",
       "                                                                                                                                                                        sentence  \\\n",
       "1  We will continue to rely in part on fossil fuels while we transition to a low-carbon economy .                                                                                  \n",
       "2  The actual rise in sea levels measured only 1.2 millimeters instead of the previously accepted 1.6 to 1.9 millimeters.                                                          \n",
       "3  Claims of global warming have been greatly exaggerated.                                                                                                                         \n",
       "4  The Intergovernmental Panel on Climate Change should be clearer on how it draws conclusions from the body of research it assesses when gauging the impacts of global warming.   \n",
       "7  Simply reducing emissions will not sufficiently limit global warming.                                                                                                           \n",
       "\n",
       "   is_high_iaa     guid  \n",
       "1  True         1_0_t1   \n",
       "2  True         1_0_t10  \n",
       "3  True         1_0_t11  \n",
       "4  True         1_0_t12  \n",
       "7  True         1_0_t15  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#write_data('high_iaa_train','',[],train_df,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Augment w/ back translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2238, 8)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtrans_fr_train_df = add_backtrans_train(train_df,'fr')\n",
    "backtrans_fr_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#write_data('high_iaa_train','',['backtrans_fr'],backtrans_fr_train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2238, 7)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtrans_zh_train_df = pd.DataFrame({\n",
    "    'round':train_df['round'].values,\n",
    "    'batch':train_df.batch.values,\n",
    "    'sent_id':train_df.sent_id.values,\n",
    "    'stance':train_df.stance.values,\n",
    "    'sentence':[get_backtrans(guid,'zh') for guid in train_df.guid],\n",
    "    'is_high_iaa':train_df.is_high_iaa.values,\n",
    "    'guid':[guid+'_zh' for guid in train_df.guid]\n",
    "})\n",
    "backtrans_zh_train_df = backtrans_zh_train_df.append(train_df,ignore_index=True)\n",
    "backtrans_zh_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#write_data('high_iaa_train','',['backtrans_zh'],backtrans_zh_train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3357, 7)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtrans_both_train_df = backtrans_zh_train_df.append(backtrans_fr_train_df,ignore_index=True).\\\n",
    "drop_duplicates('guid',keep='first')\n",
    "backtrans_both_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3357"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1119*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#write_data('high_iaa_train','',['backtrans_both'],backtrans_both_train_df,test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## All SemEval tweets as eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "semeval_df['nli_label'] = semeval_df['stance'].apply(lambda x: stance2nli[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('./save/semeval_test')\n",
    "semeval_df[['Tweet','stance','nli_label']].to_csv('./save/semeval_test'+'/test.tsv',sep='\\t',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SemEval as train, dev, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 76, 94)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semeval_df['nli_label'] = semeval_df['stance'].apply(lambda x: stance2nli[x])\n",
    "semeval_df['sentence'] = semeval_df['Tweet']\n",
    "train_ix,eval_ix = train_test_split(list(semeval_df.index),test_size=0.3,random_state=42)\n",
    "dev_ix,test_ix = train_test_split(eval_ix,test_size=0.55,random_state=42)\n",
    "len(train_ix),len(dev_ix),len(test_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((394, 5), (76, 5), (94, 5))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = semeval_df.loc[semeval_df.index.isin(train_ix)]\n",
    "dev_df = semeval_df.loc[semeval_df.index.isin(dev_ix)]\n",
    "test_df = semeval_df.loc[semeval_df.index.isin(test_ix)]\n",
    "train_df.shape,dev_df.shape,test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution:\n",
      "0    229\n",
      "1    144\n",
      "2    21 \n",
      "Name: stance, dtype: int64\n",
      "entailment       229\n",
      "neutral          144\n",
      "contradiction    21 \n",
      "Name: nli_label, dtype: int64\n",
      "\n",
      "Dev distribution:\n",
      "0    48\n",
      "1    26\n",
      "2    2 \n",
      "Name: stance, dtype: int64\n",
      "entailment       48\n",
      "neutral          26\n",
      "contradiction    2 \n",
      "Name: nli_label, dtype: int64\n",
      "\n",
      "Test distribution:\n",
      "0    58\n",
      "1    33\n",
      "2    3 \n",
      "Name: stance, dtype: int64\n",
      "0    0.617021\n",
      "1    0.351064\n",
      "2    0.031915\n",
      "Name: stance, dtype: float64\n",
      "entailment       58\n",
      "neutral          33\n",
      "contradiction    3 \n",
      "Name: nli_label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "write_data('semeval_train_eval',42,[],train_df,test_df,dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Cross-val splits (test on item-response est. label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2042"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = np.arange(len(mturk_df))\n",
    "np.random.shuffle(order)\n",
    "len(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2042 2042 205 205 1632\n",
      "2042 2042 205 205 1632\n",
      "2042 2042 204 204 1634\n",
      "2042 2042 204 204 1634\n",
      "2042 2042 204 204 1634\n",
      "2042 2042 204 204 1634\n",
      "2042 2042 204 204 1634\n",
      "2042 2042 204 204 1634\n",
      "2042 2042 204 204 1634\n",
      "2042 2042 204 204 1634\n"
     ]
    }
   ],
   "source": [
    "indices_per_fold = {}\n",
    "n_folds = 10\n",
    "for f in range(n_folds):\n",
    "    test_indices = [order[i] for i in np.arange(len(mturk_df)) if i % n_folds == f]\n",
    "    nontest_indices = list(set(np.arange(len(mturk_df))) - set(test_indices))\n",
    "    dev_indices = list(np.random.choice(nontest_indices, size=len(test_indices), replace=False))\n",
    "    train_indices = list(set(nontest_indices) - set(dev_indices))\n",
    "    all_indices = set(test_indices).union(set(dev_indices)).union(set(train_indices))\n",
    "    indices_per_fold[f] = {'train':train_indices,'dev':dev_indices,'test':test_indices}\n",
    "    print(len(all_indices), len(test_indices) + len(dev_indices) + len(train_indices), len(test_indices), len(dev_indices), len(train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(indices_per_fold,open('cross_val_10_seed_42_indices.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Vanilla MTurk (est. labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1632, 7) (205, 7) (205, 7)\n",
      "(1634, 7) (204, 7) (204, 7)\n",
      "(1634, 7) (204, 7) (204, 7)\n",
      "(1634, 7) (204, 7) (204, 7)\n",
      "(1634, 7) (204, 7) (204, 7)\n",
      "(1634, 7) (204, 7) (204, 7)\n",
      "(1634, 7) (204, 7) (204, 7)\n",
      "(1634, 7) (204, 7) (204, 7)\n",
      "(1634, 7) (204, 7) (204, 7)\n"
     ]
    }
   ],
   "source": [
    "for f in range(1,n_folds):\n",
    "    fold_0_ix = indices_per_fold[f]\n",
    "    train_ix = fold_0_ix['train']\n",
    "    test_ix = fold_0_ix['test']\n",
    "    dev_ix = fold_0_ix['dev']\n",
    "\n",
    "    train_df = mturk_df.loc[mturk_df.index.isin(train_ix)]\n",
    "    dev_df = mturk_df.loc[mturk_df.index.isin(dev_ix)]\n",
    "    test_df = mturk_df.loc[mturk_df.index.isin(test_ix)]\n",
    "    print(train_df.shape,dev_df.shape,test_df.shape)\n",
    "    #write_data('all_mturk_train_fold_{}'.format(f),42,[],train_df,test_df,dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Back translation augmented train, with and without downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add backtranslations of the train_ix examples to training\n",
    "# for f in range(0,1):\n",
    "#     fold_0_ix = indices_per_fold[f]\n",
    "#     train_ix = fold_0_ix['train']\n",
    "#     test_ix = fold_0_ix['test']\n",
    "#     dev_ix = fold_0_ix['dev']\n",
    "\n",
    "#     train_df = mturk_df.loc[mturk_df.index.isin(train_ix)]\n",
    "#     dev_df = mturk_df.loc[mturk_df.index.isin(dev_ix)]\n",
    "#     test_df = mturk_df.loc[mturk_df.index.isin(test_ix)]\n",
    "#     backtrans_fr_df = add_backtrans_train(train_df,'fr')\n",
    "#     backtrans_zh_df = add_backtrans_train(train_df,'zh')\n",
    "#     backtrans_both_df = backtrans_fr_df.append(backtrans_zh_df,ignore_index=True).drop_duplicates('guid',keep='first')\n",
    "#     print(backtrans_fr_df.shape,backtrans_zh_df.shape,backtrans_both_df.shape,dev_df.shape,test_df.shape)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_fr'],backtrans_fr_df,test_df,dev_df)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_zh'],backtrans_zh_df,test_df,dev_df)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_both'],backtrans_both_df,test_df,dev_df)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_fr'],backtrans_fr_df,test_df,dev_df,do_downsample=True)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_zh'],backtrans_zh_df,test_df,dev_df,do_downsample=True)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_both'],backtrans_both_df,test_df,dev_df,do_downsample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Back translation + upsample minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for f in range(0,1):\n",
    "#     fold_0_ix = indices_per_fold[f]\n",
    "#     train_ix = fold_0_ix['train']\n",
    "#     test_ix = fold_0_ix['test']\n",
    "#     dev_ix = fold_0_ix['dev']\n",
    "\n",
    "#     train_df = mturk_df.loc[mturk_df.index.isin(train_ix)]\n",
    "#     dev_df = mturk_df.loc[mturk_df.index.isin(dev_ix)]\n",
    "#     test_df = mturk_df.loc[mturk_df.index.isin(test_ix)]\n",
    "#     backtrans_fr_df = add_backtrans_train(train_df,'fr',upsample=True)\n",
    "#     backtrans_zh_df = add_backtrans_train(train_df,'zh',upsample=True)\n",
    "#     backtrans_both_df = backtrans_fr_df.append(backtrans_zh_df,ignore_index=True).drop_duplicates('guid',keep='first')\n",
    "#     print(backtrans_fr_df.shape,backtrans_zh_df.shape,backtrans_both_df.shape,dev_df.shape,test_df.shape)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_fr_upsampled'],backtrans_fr_df,test_df,dev_df)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_zh_upsampled'],backtrans_zh_df,test_df,dev_df)\n",
    "#     write_data('all_mturk_train_fold_{}'.format(f),seed,['backtrans_both_upsampled'],backtrans_both_df,test_df,dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCP to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paramiko import SSHClient\n",
    "from scp import SCPClient\n",
    "\n",
    "ssh = SSHClient()\n",
    "ssh.load_system_host_keys()\n",
    "ssh.connect(hostname='jacob.stanford.edu',username='yiweil',password='yldwuaeo2699zhishao15')\n",
    "\n",
    "# Define progress callback that prints the current percentage completed for the file\n",
    "def progress(filename, size, sent):\n",
    "    print(\"%s\\'s progress: %.2f%%   \\r\" % (filename, float(sent)/float(size)*100) )\n",
    "    \n",
    "cluster_data_dir = '/u/scr/yiweil/sci-debates/cc_stance/climate_data'\n",
    "local_data_dir = './save'\n",
    "\n",
    "# SCPCLient takes a paramiko transport and progress callback as its arguments.\n",
    "scp = SCPClient(ssh.get_transport(), progress=progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'dev.tsv''s progress: 0.00%   \n",
      "b'dev.tsv''s progress: 66.12%   \n",
      "b'dev.tsv''s progress: 100.00%   \n",
      "b'test.tsv''s progress: 0.00%   \n",
      "b'test.tsv''s progress: 62.81%   \n",
      "b'test.tsv''s progress: 100.00%   \n",
      "b'train.tsv''s progress: 0.00%   \n",
      "b'train.tsv''s progress: 7.75%   \n",
      "b'train.tsv''s progress: 15.51%   \n",
      "b'train.tsv''s progress: 23.26%   \n",
      "b'train.tsv''s progress: 31.01%   \n",
      "b'train.tsv''s progress: 38.77%   \n",
      "b'train.tsv''s progress: 46.52%   \n",
      "b'train.tsv''s progress: 54.28%   \n",
      "b'train.tsv''s progress: 62.03%   \n",
      "b'train.tsv''s progress: 69.78%   \n",
      "b'train.tsv''s progress: 77.54%   \n",
      "b'train.tsv''s progress: 85.29%   \n",
      "b'train.tsv''s progress: 93.04%   \n",
      "b'train.tsv''s progress: 100.00%   \n"
     ]
    }
   ],
   "source": [
    "# for file in glob.glob(local_data_dir+'/high_iaa_train_*'):\n",
    "#     scp.put(file, recursive=True, remote_path=cluster_data_dir)\n",
    "    \n",
    "for file in glob.glob(local_data_dir+'/all_mturk_*'):\n",
    "    scp.put(file, recursive=True, remote_path=cluster_data_dir)\n",
    "    break\n",
    "\n",
    "scp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
