# Climate change stance classifier

TODOs

- [ ] Training and test data
   - SemEval 2016 task 6 train, trial--both very small (211 for, 15 against)
   - SemEval 2016 test (subtask A)--169 tweets (123 for, 11 against, 35 none)
   - Dallas' annotated and predicted news data
   - Yiwei's news data--unlabeled
   - [ ] Perspectrum (Chen et al. 2019)
   - [ ] tweets? (Koenecke)
   - full news data can be used as in-domain data for initial fine-tuning
- [ ] Training classifier
   - [ ] Decide how to transform MTurk labels
   - [ ] Fine-tune on unlabeled in-domain data(; then on all in-domain data (transformer.ipynb))
   - [ ] Baselines: 
	- *BERT base uncased*
	    - train MTurk, test MTurk: 71% dev acc (yiwei: 69%) w/ macro F1: 0.68; 63% test acc w/ macro F1: 0.60
		- 8 epochs:
		    - epoch 3/8, loss=0.82; epoch 6/8, loss=0.15; epoch 8/8, loss=0.3;
		    - 69% dev acc w/ macro F1: 0.67
		    - 63% test acc w/ macro F1: 0.59
		- 6 epochs:
		    - epoch 3/8, loss=0.82; epoch 6/8, loss=0.5
		    - 70% dev acc w/ macro F1: 0.66
		- 12 epochs:
		    - epoch 3/8, loss=0.82; epoch 6/8, loss=0.17; epoch 9/12, loss=0.023; epoch 12/12, loss=0.25
		    - 70% dev acc w/ macro F1: 0.68
		- 18 epochs:
		    - epoch 3: loss=0.82, epoch 6: loss=0.14, epoch 9: loss=0.006; epoch 12: loss=0.00056; epoch 15: loss=0.00027; epoch 18: loss=0.16
		    - 70# dev acc w/ macro F1: 0.69
		- LM fine-tuned on all cc-news (cased) (perplexity 5.5): 
		    - *72% dev acc w/ macro F1: 0.70 (uncased);*
		    - 69% dev acc w/ macro F1: 66 (cased);
		    - 66% test acc w/ macro F1: 0.62 (uncased);
		    - 63% test acc w/ macro F1: 0.58 (cased)
		- LM fine-tuned on wikitext (perplexity 4.8): 
		    - 70% dev acc w/ macro F1: 0.68 (uncased); 
		    - 67% dev acc w/ macro F1: 0.65 (cased); 
		    - 63% test acc w/ macro F1: 0.59 (uncased); 
		    - 63% test acc w/ macro F1: 0.60 (cased)
	    - **train downsampled MTurk, test MTurk: 78% dev acc w/ macro F1: 0.78; 79% test acc w/ macro F1: 0.71**
		- LM fine-tuned on cc-keyword containing sentences from news (perplexity 7): 64% dev acc w/ macro F1: 0.64
		- LM fine-tuned on all cc-news (cased) (perplexity 5.5): 
		    - 62% dev acc w/ macro F1: 0.62 (uncased); 
		    - 63% dev acc w/ macro F1: 0.63 (cased);
		    - 66% test acc w/ macro F1: 0.58 (uncased);
		    - 67% test acc w/ macro F1: 0.59 (cased)
		- LM fine-tuned on wikitext (perplexity 4.8): 
		    - 66% dev acc w/ macro F1: 0.66 (uncased); 
		    - 60% dev acc w/ macro F1: 0.60 (cased); 
		    - 65% test acc w/ macro F1: 0.56 (uncased); 
		    - 62% test acc w/ macro F1: 0.53 (cased)
	    - *train flattened MTurk, test MTurk: 67% dev acc w/ macro F1: 0.64; 74% test acc w/ macro F1: 0.73*
	    - train flattened downsampled MTurk, test Mturk: 63% dev acc w/ macro F1 0.62; 64% test acc w/ macro F1: 0.63
	    - train windowed (n=2) MTurk, test regular MTurk: 52% dev acc w/ macro F1: 0.51; 55% test acc w/ macro F1: 0.55
	    - train windowed (n=1) MTurk, test regular MTurk: 55% dev acc w/ macro F1: 0.54; 56% test acc w/ macro F1: 0.54; test downsampled MTurk: 66% test acc w/ macro F1: 0.55, 64% dev acc w/ macro F1: 0.64
	    - train windowed (n=1) MTurk + downsampling, test regular MTurk: 52% dev acc w/ macro F1: 0.50; 52% test acc w/ macro F1: 0.50
	    - train all, test MTurk: 80% dev acc w/ Macro F1: 0.55; 50% test acc w/ Macro F1: 0.47 (sharp decrease from dev to test acc due to test data being MTurk only)
	    - train downsampled all, test MTurk: 50% dev acc w/ macro F1: 0.31; 46% test acc w/ macro F1: 0.44
	    - train MTurk, test twint: 7% test acc 
	- BERT base cased
	    - train MTurk, test MTurk: 65% dev acc w/ macro F1: 0.61
	    - train downsampled MTurk, test MTurk: 64% dev acc w/ macro F1: 0.64
	- BERT large uncased
	    - train MTurk, test MTurk: 73% dev acc w/ macro F1: 0.71, 68% test acc w/ macro F1: 0.65
	    - train downsampled MTurk, test MTurk: 70% dev acc w/ macro F1: 0.70, 67% test acc w/ macro F1: 0.58
	- GPT 
	    - all labeled data: 79% dev acc
	    - MTurk only: 69% dev acc
	    - train all; test MTurk: 80.9% test acc; Macro F1: 0.30
	
	- GPT-2
	- XLNet
	    - poor (acc 39%)
	- *RoBERTa*
	    - MTurk only: 72% dev acc; macro F1: 0.72
	- DistilBERT
	    - MTurk only: 69% acc; macro F1: 0.66
	- AlBERT
	    - MTurk only: 64% acc; macro F1: 0.61
	- FlauBERT
	    - MTurk only: 50% acc; macro F1: 0.37
	- STANCY on perspectrum (https://www.aclweb.org/anthology/D19-1675.pdf)
	    - swap out their C for our single target sentence
	    - instead of BERT, use GPT
   - [ ] Error analysis
	- Evaluation on test data (change get_dev -> get_test in run.py)
	- Need to upsample "disagree"
	- Change to ordinal loss function
	- Look into BERT for text classification
- [x] MTurk 
   - [x] pilot, iterate with smaller subsample to check inter-annotator agreement
   - [x] Lock down non-pilot specifics:
	- [x] Config settings: 98% minPercentPreviousHITsApproved, 1000 minNumPreviousHITsApproved, USonly = yes
	- [x] Collect 2,000 total annotations in 5 rounds: N=300, 400, 400, 450, 450
	- [x] 8 annotators per annotation item
	- [x] Each HIT will have N/10 true items to annotate + 5 screen items (screen items differ b/w rounds, so need 25 total) (so Round 1 will have 35; Round 2, 45; Round 3, 50)
	- [x] N items/(N/10 items/HIT) = 10 HITs per round; multiply by 6 annotators = 60 annotators paid
	- [x] Payment (based on $12/hr MW in CA): $4, $5.14, $5.7 for Rounds 1, 2, 3, 4, 5; total cost = $4*60+$5.14*120+$5.7*120 = $1,540.8 USD
	- [ ] Exclusion criteria for all rounds, after collection:
		- [ ] Turkers who choose agrees/disagrees on screen Q for which answer is disagrees/agrees
		- [ ] For each Turker, calculate %items for which all other 5 Turkers chose agrees/disagrees but they chose disagrees/agrees; if this % is greater than N--exclude. N>=10%?
		- [ ] If find an effect from party--drop annotations s.t. balance of annotations per item from D and R
		- [ ] increase task size, check IRR b/w first and second half of exp (60)
   - [x] Analysis:
	- [x] Item length effect on rating
	- [x] average rating for each item, divided by liberal vs. conservative sources


- Miscellaneous ideas:
	- Track whether people decide to click for more context--signal of the ambiguity of the sentence (medium priority)
	- Question mark button for more information about abbreviations? (Can we track whether someone's clicked on a question mark?)
	- Future study in which I manipulate stance-taking verb
            - How do you mix them? Across, within speaker thing?

References:
   - https://towardsdatascience.com/transfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde
   - STANCY https://www.aclweb.org/anthology/D19-1675.pdf
   - http://www.saifmohammad.com/WebDocs/StarSem2016-stance-tweets.pdf

