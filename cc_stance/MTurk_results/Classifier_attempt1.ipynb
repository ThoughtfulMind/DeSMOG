{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import operator\n",
    "import mpl_toolkits\n",
    "mpl_toolkits.__path__.append('/usr/lib/python2.7/dist-packages/mpl_toolkits/')\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from numpy.random import RandomState\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from datetime import timedelta  \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#from tabulate import tabulate\n",
    "\n",
    "from keras import Sequential, backend, regularizers\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import train, test, validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837 205\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "training_data = pd.read_csv(os.getcwd()+'/data_for_classifier/dedup_training_data.csv')\n",
    "val_data = pd.read_csv(os.getcwd()+'/data_for_classifier/dedup_val_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label = {'0':'agree','1':'disagree','2':'neutral','3':'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {'sent':[],'label':[],'source':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for med_bias in ['anti','pro']:\n",
    "    with open(os.path.join('../','test_data/{}-cc/test_data_text_only.txt'.format(med_bias)))\\\n",
    "    as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if len(line) > 0:\n",
    "                split_line = line.split('\\t\\t')\n",
    "                text = split_line[0]\n",
    "                label = split_line[-1]\n",
    "                if len(label) == 1:\n",
    "                    str_label = int2label[label]\n",
    "                    source = med_bias\n",
    "                    test_data['sent'].append(text)\n",
    "                    test_data['label'].append(str_label)\n",
    "                    test_data['source'].append(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pro     445\n",
       "anti    384\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     627\n",
       "agree       129\n",
       "disagree     73\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>We ’re also doing some avian and marine mammal...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>In preconstruction monitoring , we ’re looking...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>“ This project will have to address the marine...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>N.J. “ It is a project with a lot of potential...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>that studies need to be done</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>Thus , as in any facet of life , we must make ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>whether a doubling of CO2 will ultimately resu...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>what is a ridiculously complex phenomenon into...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>Even if we know some of these are far more imp...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>that it is playing some role</td>\n",
       "      <td>neutral</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>829 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sent    label source\n",
       "0    We ’re also doing some avian and marine mammal...  neutral   anti\n",
       "1    In preconstruction monitoring , we ’re looking...  neutral   anti\n",
       "2    “ This project will have to address the marine...  neutral   anti\n",
       "3    N.J. “ It is a project with a lot of potential...  neutral   anti\n",
       "4                         that studies need to be done  neutral   anti\n",
       "..                                                 ...      ...    ...\n",
       "824  Thus , as in any facet of life , we must make ...  neutral    pro\n",
       "825  whether a doubling of CO2 will ultimately resu...  neutral    pro\n",
       "826  what is a ridiculously complex phenomenon into...  neutral    pro\n",
       "827  Even if we know some of these are far more imp...  neutral    pro\n",
       "828                       that it is playing some role  neutral    pro\n",
       "\n",
       "[829 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837 205 829\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data), len(val_data), len(test_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the corpus:  1837\n"
     ]
    }
   ],
   "source": [
    "#generate the corpus for training and validation data that will be use for different featurizations\n",
    "corpus_all = training_data[pd.isnull(training_data['sent_clean'])==False]\n",
    "corpus_all.reset_index(inplace=True)\n",
    "corpus_all.to_csv('corpus.csv', sep=',')\n",
    "corpus_all = corpus_all[pd.isnull(corpus_all['max_prob_label'])==False]\n",
    "corpus_all.reset_index(inplace=True)\n",
    "corpus = corpus_all['sent_clean']\n",
    "corpus_val = val_data[pd.isnull(val_data['sent_clean'])==False]\n",
    "corpus_val.reset_index(inplace=True)\n",
    "corpus_val = corpus_val[pd.isnull(corpus_val['max_prob_label'])==False]\n",
    "corpus_val.reset_index(inplace=True)\n",
    "print(\"length of the corpus: \",len(corpus))\n",
    "\n",
    "# outcomes for RNN\n",
    "Y_train = corpus_all['max_prob_label']\n",
    "Y_val = corpus_val['max_prob_label']\n",
    "Y_test = test_data_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Featurization methods: NB, k-means, SVM (rbf and linear kernel), RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic unigram featurization of tweets that are not in the validation/test set \n",
    "# Find unigram features\n",
    "vectorizerCount = CountVectorizer(stop_words='english',binary=True)\n",
    "train_unigram = vectorizerCount.fit_transform(corpus).toarray()\n",
    "test_unigram = vectorizerCount.transform(test_data_df['sent']).toarray()\n",
    "val_unigram = vectorizerCount.transform(corpus_val['sent_clean']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Bernoulli naive bayes\n",
    "bnb_uni = BernoulliNB(alpha=1)\n",
    "bnb_uni.fit(train_unigram, corpus_all['max_prob_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy:   0.634\n",
      "test accuracy:   0.698\n"
     ]
    }
   ],
   "source": [
    "#validation and test error for Bernouilli Naive Bayes\n",
    "pred_val = bnb_uni.predict(val_unigram)\n",
    "score_val_u1 = metrics.accuracy_score(corpus_val['max_prob_label'], pred_val)\n",
    "pred = bnb_uni.predict(test_unigram)\n",
    "score_u1 = metrics.accuracy_score(test_data_df['label'], pred)\n",
    "print(\"validation accuracy:   %0.3f\" % score_val_u1)\n",
    "print(\"test accuracy:   %0.3f\" % score_u1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy:   0.000\n",
      "test accuracy:   0.000\n"
     ]
    }
   ],
   "source": [
    "# perform k-means (with k=2) in test and validation sets\n",
    "kmeans_val = KMeans(n_clusters=2, init = 'random', n_init = 10).fit(val_unigram)\n",
    "kmeans = KMeans(n_clusters=2, init = 'random', n_init = 10).fit(test_unigram)\n",
    "\n",
    "# Check errors with k-means\n",
    "pred1_val = kmeans_val.labels_*(2)-1\n",
    "pred2_val = pred1_val*(-1)\n",
    "score1_val = metrics.accuracy_score(corpus_val['max_prob_label'], pred1_val)\n",
    "score2_val = metrics.accuracy_score(corpus_val['max_prob_label'], pred2_val)\n",
    "score_val_u2 = max(score1_val,score2_val)\n",
    "print(\"validation accuracy:   %0.3f\" % score_val_u2)\n",
    "\n",
    "pred1 = kmeans.labels_*(2)-1\n",
    "pred2 = pred1*(-1)\n",
    "score1 = metrics.accuracy_score(test_data_df['label'], pred1)\n",
    "score2 = metrics.accuracy_score(test_data_df['label'], pred2)\n",
    "score_u2 = max(score1,score2)\n",
    "print(\"test accuracy:   %0.3f\" % score_u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy:   0.605\n",
      "test accuracy:   0.683\n"
     ]
    }
   ],
   "source": [
    "# Classify using linear SVM\n",
    "clf = svm.SVC(kernel='linear') #gamma='scale'\n",
    "clf.fit(train_unigram, corpus_all['max_prob_label'])  \n",
    "pred_val = clf.predict(val_unigram)\n",
    "score_val_u4 = metrics.accuracy_score(corpus_val['max_prob_label'], pred_val)\n",
    "pred = clf.predict(test_unigram)\n",
    "score_u4 = metrics.accuracy_score(test_data_df['label'], pred)\n",
    "print(\"validation accuracy:   %0.3f\" % score_val_u4)\n",
    "print(\"test accuracy:   %0.3f\" % score_u4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/Users/yiweiluo/Downloads/datasets/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[0:len(values)-300])\n",
    "    try:\n",
    "        coefs = np.asarray(values[len(values)-300:], dtype='float32')\n",
    "    except ValueError:\n",
    "        print(line)\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "\n",
    "voc_size = 8000\n",
    "max_words = 50\n",
    "tokenizer = Tokenizer(num_words=voc_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "X_train = tokenizer.texts_to_sequences(corpus)\n",
    "X_train = pad_sequences(X_train,maxlen=max_words)\n",
    "X_val = tokenizer.texts_to_sequences(corpus_val['tweet_clean'])\n",
    "X_val = pad_sequences(X_val,maxlen=max_words)\n",
    "X_test = tokenizer.texts_to_sequences(test_tweets['tweet_clean'])\n",
    "X_test = pad_sequences(X_test,maxlen=max_words)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "MAX_SEQUENCE_LENGTH=max_words\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH\n",
    "                            #,\n",
    "                            #trainable=False\n",
    "                           )\n",
    "\n",
    "lstm_size = 20\n",
    "dense_layer = 250\n",
    "backend.clear_session()\n",
    "model=Sequential()\n",
    "model.add(embedding_layer)# input_length=max_words))#,dropout = 0.2\n",
    "#model.add(Dense(dense_layer, activation='relu'))\n",
    "#model.add(LSTM(lstm_size,return_sequences=True, dropout=0.2, recurrent_dropout=0.2, activation='relu'))\n",
    "#model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(lstm_size,dropout=0.4, recurrent_dropout=0.4, activation='relu'))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "#X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "#X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_val_t5 = history.history['val_accuracy'][-1]\n",
    "score_t5 = model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "\n",
    "print(\"validation accuracy:   %0.3f\" % score_val_t5)\n",
    "print(\"test accuracy:   %0.3f\" % score_t5)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.round(model.predict(X_test))\n",
    "X_pred[X_pred>1]=1\n",
    "X_pred[X_pred<=0]=-1\n",
    "\n",
    "print(test_tweets['label'].shape)\n",
    "print(X_pred.shape)\n",
    "test_compare = test_tweets\n",
    "test_compare['pred'] = X_pred\n",
    "test_compare\n",
    "\n",
    "same = test_compare[test_compare['label']==test_compare['pred']]\n",
    "diff_falsepos = test_compare[test_compare['label']<test_compare['pred']]\n",
    "diff_falseneg = test_compare[test_compare['label']>test_compare['pred']]\n",
    "print(len(same), len(diff_falsepos), len(diff_falseneg))\n",
    "\n",
    "diff_falseneg.to_csv('FalseNeg.csv', sep=',')\n",
    "diff_falsepos.to_csv('FalsePos.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
