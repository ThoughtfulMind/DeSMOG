{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, AdamW, WEIGHTS_NAME\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "#from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import argparse\n",
    "import scipy\n",
    "import sklearn\n",
    "import math\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, confusion_matrix\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CUDA = (torch.cuda.device_count() > 0)\n",
    "CLASSES = ['agrees','neutral','disagrees'] # correspond to 0, 1, 2 in labeled data\n",
    "NUM_LABELS = 3\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    micro_f1 = f1_score(y_true=labels, y_pred=preds, average='micro')\n",
    "    macro_f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_f1\":macro_f1,\n",
    "        \"acc_and_macro_f1\": (acc + macro_f1) / 2,\n",
    "    }\n",
    "\n",
    "def cm(preds, labels):\n",
    "    return confusion_matrix(preds, labels)\n",
    "\n",
    "def get_pred_label(res_,to_str=False):\n",
    "    if to_str:\n",
    "        return CLASSES[res_.index(max(res_))]\n",
    "    else:\n",
    "        return res_.index(max(res_))\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def save_pretrained(model, save_directory):\n",
    "    \"\"\" Save a model and its configuration file to a directory, so that it\n",
    "        can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
    "    \"\"\"\n",
    "    assert os.path.isdir(\n",
    "        save_directory\n",
    "    ), \"Saving path should be a directory where the model and configuration can be saved\"\n",
    "\n",
    "    # Only save the model itself if we are using distributed training\n",
    "    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "    # Attach architecture to the config\n",
    "    model_to_save.config.architectures = [model_to_save.__class__.__name__]\n",
    "\n",
    "    # Save configuration file\n",
    "    model_to_save.config.save_pretrained(save_directory)\n",
    "    # with open(os.path.join(save_directory,'config.json'), 'w') as outfile:\n",
    "    #     json.dump(model_to_save.config, outfile)\n",
    "\n",
    "    # If we save using the predefined names, we can load using `from_pretrained`\n",
    "    output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    logger.info(\"Model weights saved in {}\".format(output_model_file))\n",
    "\n",
    "\n",
    "def build_dataloader(*args, sampler='random'):\n",
    "    #print(args[:2])\n",
    "    data = (torch.tensor(x) for x in args)\n",
    "    #print(data[0])\n",
    "    data = TensorDataset(*data)\n",
    "    \n",
    "    sampler = RandomSampler(data) if sampler == 'random' else SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=1)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "def get_out_data(dat_path,max_seq_length=500):\n",
    "    #eval_set = 'train' # can also be 'test'\n",
    "    data = pd.read_csv(dat_path,\n",
    "                              sep='\\t',header=None)\n",
    "    data.columns = ['text','label']#,'outlet']\n",
    "\n",
    "    out = defaultdict(list)\n",
    "\n",
    "    print('Number of examples:',len(data))\n",
    "    to_predict = data.text.values\n",
    "    true = data.label.values\n",
    "\n",
    "    for dat_ix in range(0,len(data)):\n",
    "        sent = to_predict[dat_ix]\n",
    "        #print(sent)\n",
    "        label = true[dat_ix]\n",
    "        encoded_sent = tokenizer.encode(sent,add_special_tokens=True)[1:] # remove [CLS] auto-inserted at beginning\n",
    "        #print('encoded sent:',encoded_sent)\n",
    "        CLS_ix = encoded_sent.index(101)\n",
    "        SEP_ix = encoded_sent[CLS_ix:].index(102)+CLS_ix\n",
    "        out['input_ids'].append(encoded_sent)\n",
    "        out['sentences'].append(sent)\n",
    "        out['label'].append(label)\n",
    "        out['index_CLS'].append(CLS_ix)\n",
    "        out['index_SEP_after_CLS'].append(SEP_ix)\n",
    "        #print(encoded_sent[CLS_ix])\n",
    "\n",
    "    out['input_ids'] = pad_sequences(\n",
    "            out['input_ids'],\n",
    "            maxlen=max_seq_length,\n",
    "            dtype=\"long\",\n",
    "            value=0,\n",
    "            truncating=\"post\",\n",
    "            padding=\"post\")\n",
    "\n",
    "\n",
    "    print('Adding attention masks...')\n",
    "    # get attn masks\n",
    "    for sent_no,sent in enumerate(out['input_ids']):\n",
    "        tok_type_ids = [0 for tok_id in sent]\n",
    "        #print('tok type ids:',tok_type_ids\n",
    "        #     )\n",
    "        #mask = [int(tok_id > 0) for tok_id in sent]\n",
    "        #print('old mask:',mask)\n",
    "        #print('CLS index:',out['index_CLS'][sent_no])\n",
    "        #print('SEP index:',out['index_SEP_after_CLS'][sent_no])\n",
    "        mask = [0 if n < out['index_CLS'][sent_no] or n > out['index_SEP_after_CLS'][sent_no] else 1 \n",
    "                for n,tok_id in enumerate(sent)]\n",
    "        out['attention_mask'].append(mask)\n",
    "        out['token_type_ids'].append(tok_type_ids)\n",
    "    #print(len(out['labels']))\n",
    "    #print(sum(out['labels']))\n",
    "\n",
    "    print('Preparing input examples for prediction...')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name/path: ../BERT/LM_finetuned/uncased_LM_cc_output\n",
      "data dir: ../data_creation/scripts/save/mturk_windowed_1\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = './'\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "SEED = 420\n",
    "DATA_NAME = 'mturk_windowed_1'\n",
    "DATA_DIR = os.path.join('../data_creation/scripts/save',DATA_NAME)\n",
    "BASE_MODEL = 'uncased_LM_cc_output'\n",
    "MODELS_DIR = '../BERT/LM_finetuned'\n",
    "MODEL_NAME_OR_PATH = os.path.join(MODELS_DIR,BASE_MODEL)\n",
    "print('model name/path:',MODEL_NAME_OR_PATH)\n",
    "EVAL_ON_TEST = False\n",
    "PRED_FILE_NAME = 'dev_preds'\n",
    "DO_TRAIN = True\n",
    "DO_EVAL = True\n",
    "LOCAL_RANK = -1\n",
    "NO_CUDA = False\n",
    "print('data dir:',DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "code_folding": [
     0,
     5,
     13,
     16,
     24
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/15/2020 13:43:55 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/yiweiluo/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "04/15/2020 13:43:55 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/15/2020 13:43:55 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/yiweiluo/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "04/15/2020 13:43:55 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config...\n",
      "Config path: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/15/2020 13:43:55 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/yiweiluo/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "04/15/2020 13:43:58 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "04/15/2020 13:43:58 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "04/15/2020 13:43:58 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/yiweiluo/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (loss_fct): MultiLabelMarginLoss()\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (\n",
    "        os.path.exists(OUTPUT_DIR)\n",
    "        and os.listdir(OUTPUT_DIR)\n",
    "        and DO_TRAIN\n",
    "        and not OUTPUT_DIR\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            OUTPUT_DIR\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if LOCAL_RANK == -1 or NO_CUDA:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not NO_CUDA else \"cpu\")\n",
    "    N_GPU = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(LOCAL_RANK)\n",
    "    device = torch.device(\"cuda\", LOCAL_RANK)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    N_GPU = 1\n",
    "DEVICE = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if LOCAL_RANK in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Load model\n",
    "# config = BertConfig.from_pretrained(MODEL_NAME_OR_PATH, num_labels=NUM_LABELS)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_OR_PATH,\n",
    "#                                                           config=config)\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)#ARGS.casing)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_ix, v = [],[]\n",
    "with open('vocab.txt-vocab.txt','r') as f:\n",
    "    for ix,line in enumerate(f):\n",
    "        v_ix.append(ix)\n",
    "        v.append(line.strip())\n",
    "vocab_dict = dict(zip(v_ix,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 130\n",
      "Adding attention masks...\n",
      "Preparing input examples for prediction...\n"
     ]
    }
   ],
   "source": [
    "# Load data for prediction/eval\n",
    "eval_set = 'test' if EVAL_ON_TEST else 'dev' # can also be 'test'\n",
    "eval_dat_path = os.path.join(DATA_DIR,eval_set+'.tsv')\n",
    "eval_data = get_out_data(eval_dat_path,max_seq_length=500)\n",
    "\n",
    "test_inputs, test_labels, test_masks = eval_data['input_ids'], eval_data['label'], eval_data['attention_mask']\n",
    "test_dataloader = build_dataloader(\n",
    "    test_inputs, test_labels, test_masks,\n",
    "    sampler='order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  102,  2037,  2817,  1010,  2405,  6928,  1999,  1996,  3485,\n",
       "        3267,  4785,  2689,  1010,  2758,  2008,  2087,  2436,  3121,\n",
       "        2275,  7715,  2241,  2006,  1037,  5109,  1011,  2214,  5675,\n",
       "        2008,  3594,  1996, 21453,  6165,  1997,  2273,  1012,   102,\n",
       "         101,  3121,  2323,  2024,  8566,  3401,  5907,  1011,  5860,\n",
       "       20026, 19185, 13827,  1999,  9829,  7216,  2050,  2138,  4292,\n",
       "        7715,  2012,  3621, 16676,  3798,  2064,  2393,  4337,  3795,\n",
       "       12959,  1012,   102,  1523,  1999,  1037,  2843,  1997,  3121,\n",
       "        1010,  2017,  2156,  2943,  8381,  2003,  1037,  2843,  3020,\n",
       "        2138,  1996,  3115,  2003, 10250, 12322,  9250,  2005,  2273,\n",
       "        1521,  1055,  2303,  3684,  2537,  1010,  1524,  2056, 11235,\n",
       "        2332,  2863,  1010,  1037,  2522,  1011,  3166,  1997,  1996,\n",
       "        2817,  1998,  1037, 16012, 21281, 19570,  2923,  2012,  5003,\n",
       "       14083, 13149,  2102,  2118,  2966,  2415,  1999,  1996,  4549,\n",
       "        1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data_creation/scripts/save/mturk_windowed_1/dev.tsv'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('../data_creation/scripts/save/mturk_windowed_1/train.tsv',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1495, 2)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 556, 1: 618, 2: 321})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/15/2020 13:26:58 - INFO - __main__ -   Saving model checkpoint to ./\n",
      "04/15/2020 13:26:58 - INFO - transformers.configuration_utils -   Configuration saved in ./config.json\n",
      "04/15/2020 13:26:59 - INFO - __main__ -   Model weights saved in ./pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 1495\n",
      "Adding attention masks...\n",
      "Preparing input examples for prediction...\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "if DO_TRAIN:\n",
    "    if not os.path.exists(OUTPUT_DIR) and LOCAL_RANK in [-1, 0]:\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", OUTPUT_DIR)\n",
    "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "    # They can then be reloaded using `from_pretrained()`\n",
    "    model_to_save = (\n",
    "        model.module if hasattr(model, \"module\") else model\n",
    "    )  # Take care of distributed/parallel training\n",
    "    save_pretrained(model_to_save,OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    #torch.save(ARGS, os.path.join(ARGS.output_dir, \"training_args.bin\"))\n",
    "\n",
    "#     if os.path.exists(OUTPUT_DIR + \"/data.cache.pkl\"):\n",
    "#         data = pickle.load(open(OUTPUT_DIR + \"/data.cache.pkl\", 'rb'))\n",
    "#     else:\n",
    "    data = get_out_data(os.path.join(DATA_DIR, 'train.tsv'))\n",
    "    pickle.dump(data, open(OUTPUT_DIR + \"/data.cache.pkl\", 'wb'))\n",
    "\n",
    "    train_inputs, train_labels, train_masks = data['input_ids'], data['label'], data['attention_mask']\n",
    "    train_dataloader = build_dataloader(\n",
    "        train_inputs, train_labels, train_masks)\n",
    "\n",
    "    total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([2])\n",
      "1 tensor([0])\n",
      "2 tensor([0])\n",
      "3 tensor([1])\n",
      "4 tensor([0])\n",
      "5 tensor([1])\n",
      "6 tensor([1])\n",
      "7 tensor([2])\n",
      "8 tensor([0])\n",
      "9 tensor([0])\n",
      "10 tensor([2])\n",
      "11 tensor([1])\n",
      "12 tensor([1])\n",
      "13 tensor([1])\n",
      "14 tensor([2])\n",
      "15 tensor([2])\n",
      "16 tensor([1])\n",
      "17 tensor([1])\n",
      "18 tensor([2])\n",
      "19 tensor([1])\n",
      "20 tensor([1])\n",
      "21 tensor([0])\n",
      "22 tensor([0])\n",
      "23 tensor([0])\n",
      "24 tensor([1])\n",
      "25 tensor([2])\n",
      "26 tensor([0])\n",
      "27 tensor([0])\n",
      "28 tensor([1])\n",
      "29 tensor([1])\n",
      "30 tensor([2])\n",
      "31 tensor([2])\n",
      "32 tensor([1])\n",
      "33 tensor([0])\n",
      "34 tensor([1])\n",
      "35 tensor([1])\n",
      "36 tensor([2])\n",
      "37 tensor([0])\n",
      "38 tensor([1])\n",
      "39 tensor([1])\n",
      "40 tensor([1])\n",
      "41 tensor([0])\n",
      "42 tensor([1])\n",
      "43 tensor([2])\n",
      "44 tensor([0])\n",
      "45 tensor([1])\n",
      "46 tensor([1])\n",
      "47 tensor([1])\n",
      "48 tensor([1])\n",
      "49 tensor([1])\n",
      "50 tensor([0])\n",
      "51 tensor([1])\n",
      "52 tensor([2])\n",
      "53 tensor([2])\n",
      "54 tensor([1])\n",
      "55 tensor([1])\n",
      "56 tensor([2])\n",
      "57 tensor([1])\n",
      "58 tensor([1])\n",
      "59 tensor([0])\n",
      "60 tensor([1])\n",
      "61 tensor([0])\n",
      "62 tensor([1])\n",
      "63 tensor([0])\n",
      "64 tensor([2])\n",
      "65 tensor([1])\n",
      "66 tensor([1])\n",
      "67 tensor([1])\n",
      "68 tensor([2])\n",
      "69 tensor([2])\n",
      "70 tensor([1])\n",
      "71 tensor([0])\n",
      "72 tensor([0])\n",
      "73 tensor([2])\n",
      "74 tensor([2])\n",
      "75 tensor([0])\n",
      "76 tensor([0])\n",
      "77 tensor([2])\n",
      "78 tensor([1])\n",
      "79 tensor([1])\n",
      "80 tensor([0])\n",
      "81 tensor([0])\n",
      "82 tensor([1])\n",
      "83 tensor([2])\n",
      "84 tensor([0])\n",
      "85 tensor([2])\n",
      "86 tensor([1])\n",
      "87 tensor([0])\n",
      "88 tensor([1])\n",
      "89 tensor([0])\n",
      "90 tensor([0])\n",
      "91 tensor([1])\n",
      "92 tensor([2])\n",
      "93 tensor([1])\n",
      "94 tensor([0])\n",
      "95 tensor([0])\n",
      "96 tensor([0])\n",
      "97 tensor([2])\n",
      "98 tensor([0])\n",
      "99 tensor([1])\n",
      "100 tensor([1])\n",
      "101 tensor([2])\n",
      "102 tensor([2])\n",
      "103 tensor([1])\n",
      "104 tensor([1])\n",
      "105 tensor([1])\n",
      "106 tensor([1])\n",
      "107 tensor([2])\n",
      "108 tensor([0])\n",
      "109 tensor([2])\n",
      "110 tensor([1])\n",
      "111 tensor([0])\n",
      "112 tensor([1])\n",
      "113 tensor([2])\n",
      "114 tensor([0])\n",
      "115 tensor([0])\n",
      "116 tensor([1])\n",
      "117 tensor([0])\n",
      "118 tensor([0])\n",
      "119 tensor([1])\n",
      "120 tensor([0])\n",
      "121 tensor([0])\n",
      "122 tensor([0])\n",
      "123 tensor([0])\n",
      "124 tensor([1])\n",
      "125 tensor([1])\n",
      "126 tensor([1])\n",
      "127 tensor([2])\n",
      "128 tensor([1])\n",
      "129 tensor([2])\n",
      "130 tensor([0])\n",
      "131 tensor([1])\n",
      "132 tensor([1])\n",
      "133 tensor([0])\n",
      "134 tensor([0])\n",
      "135 tensor([0])\n",
      "136 tensor([0])\n",
      "137 tensor([2])\n",
      "138 tensor([0])\n",
      "139 tensor([0])\n",
      "140 tensor([2])\n",
      "141 tensor([1])\n",
      "142 tensor([2])\n",
      "143 tensor([1])\n",
      "144 tensor([1])\n",
      "145 tensor([1])\n",
      "146 tensor([0])\n",
      "147 tensor([2])\n",
      "148 tensor([0])\n",
      "149 tensor([1])\n",
      "150 tensor([0])\n",
      "151 tensor([2])\n",
      "152 tensor([0])\n",
      "153 tensor([0])\n",
      "154 tensor([1])\n",
      "155 tensor([0])\n",
      "156 tensor([0])\n",
      "157 tensor([0])\n",
      "158 tensor([2])\n",
      "159 tensor([0])\n",
      "160 tensor([1])\n",
      "161 tensor([1])\n",
      "162 tensor([0])\n",
      "163 tensor([1])\n",
      "164 tensor([1])\n",
      "165 tensor([1])\n",
      "166 tensor([0])\n",
      "167 tensor([0])\n",
      "168 tensor([1])\n",
      "169 tensor([1])\n",
      "170 tensor([0])\n",
      "171 tensor([1])\n",
      "172 tensor([0])\n",
      "173 tensor([2])\n",
      "174 tensor([2])\n",
      "175 tensor([0])\n",
      "176 tensor([0])\n",
      "177 tensor([0])\n",
      "178 tensor([1])\n",
      "179 tensor([0])\n",
      "180 tensor([0])\n",
      "181 tensor([0])\n",
      "182 tensor([0])\n",
      "183 tensor([0])\n",
      "184 tensor([0])\n",
      "185 tensor([0])\n",
      "186 tensor([0])\n",
      "187 tensor([1])\n",
      "188 tensor([2])\n",
      "189 tensor([0])\n",
      "190 tensor([0])\n",
      "191 tensor([0])\n",
      "192 tensor([0])\n",
      "193 tensor([1])\n",
      "194 tensor([2])\n",
      "195 tensor([0])\n",
      "196 tensor([2])\n",
      "197 tensor([0])\n",
      "198 tensor([2])\n",
      "199 tensor([0])\n",
      "200 tensor([1])\n",
      "201 tensor([0])\n",
      "202 tensor([0])\n",
      "203 tensor([1])\n",
      "204 tensor([0])\n",
      "205 tensor([0])\n",
      "206 tensor([2])\n",
      "207 tensor([1])\n",
      "208 tensor([0])\n",
      "209 tensor([1])\n",
      "210 tensor([0])\n",
      "211 tensor([2])\n",
      "212 tensor([0])\n",
      "213 tensor([2])\n",
      "214 tensor([1])\n",
      "215 tensor([1])\n",
      "216 tensor([0])\n",
      "217 tensor([1])\n",
      "218 tensor([2])\n",
      "219 tensor([2])\n",
      "220 tensor([0])\n",
      "221 tensor([1])\n",
      "222 tensor([0])\n",
      "223 tensor([1])\n",
      "224 tensor([0])\n",
      "225 tensor([0])\n",
      "226 tensor([0])\n",
      "227 tensor([1])\n",
      "228 tensor([2])\n",
      "229 tensor([2])\n",
      "230 tensor([0])\n",
      "231 tensor([0])\n",
      "232 tensor([0])\n",
      "233 tensor([1])\n",
      "234 tensor([1])\n",
      "235 tensor([2])\n",
      "236 tensor([1])\n",
      "237 tensor([1])\n",
      "238 tensor([1])\n",
      "239 tensor([0])\n",
      "240 tensor([1])\n",
      "241 tensor([1])\n",
      "242 tensor([0])\n",
      "243 tensor([0])\n",
      "244 tensor([0])\n",
      "245 tensor([1])\n",
      "246 tensor([0])\n",
      "247 tensor([0])\n",
      "248 tensor([0])\n",
      "249 tensor([1])\n",
      "250 tensor([2])\n",
      "251 tensor([1])\n",
      "252 tensor([1])\n",
      "253 tensor([1])\n",
      "254 tensor([1])\n",
      "255 tensor([2])\n",
      "256 tensor([0])\n",
      "257 tensor([1])\n",
      "258 tensor([0])\n",
      "259 tensor([0])\n",
      "260 tensor([0])\n",
      "261 tensor([0])\n",
      "262 tensor([1])\n",
      "263 tensor([0])\n",
      "264 tensor([2])\n",
      "265 tensor([0])\n",
      "266 tensor([0])\n",
      "267 tensor([2])\n",
      "268 tensor([1])\n",
      "269 tensor([2])\n",
      "270 tensor([1])\n",
      "271 tensor([0])\n",
      "272 tensor([1])\n",
      "273 tensor([0])\n",
      "274 tensor([0])\n",
      "275 tensor([0])\n",
      "276 tensor([0])\n",
      "277 tensor([1])\n",
      "278 tensor([1])\n",
      "279 tensor([1])\n",
      "280 tensor([0])\n",
      "281 tensor([1])\n",
      "282 tensor([2])\n",
      "283 tensor([2])\n",
      "284 tensor([1])\n",
      "285 tensor([1])\n",
      "286 tensor([1])\n",
      "287 tensor([0])\n",
      "288 tensor([0])\n",
      "289 tensor([1])\n",
      "290 tensor([0])\n",
      "291 tensor([0])\n",
      "292 tensor([1])\n",
      "293 tensor([0])\n",
      "294 tensor([0])\n",
      "295 tensor([0])\n",
      "296 tensor([1])\n",
      "297 tensor([1])\n",
      "298 tensor([0])\n",
      "299 tensor([0])\n",
      "300 tensor([1])\n",
      "301 tensor([1])\n",
      "302 tensor([0])\n",
      "303 tensor([2])\n",
      "304 tensor([1])\n",
      "305 tensor([0])\n",
      "306 tensor([1])\n",
      "307 tensor([0])\n",
      "308 tensor([0])\n",
      "309 tensor([1])\n",
      "310 tensor([0])\n",
      "311 tensor([1])\n",
      "312 tensor([1])\n",
      "313 tensor([0])\n",
      "314 tensor([1])\n",
      "315 tensor([2])\n",
      "316 tensor([1])\n",
      "317 tensor([1])\n",
      "318 tensor([1])\n",
      "319 tensor([0])\n",
      "320 tensor([2])\n",
      "321 tensor([0])\n",
      "322 tensor([1])\n",
      "323 tensor([2])\n",
      "324 tensor([0])\n",
      "325 tensor([1])\n",
      "326 tensor([2])\n",
      "327 tensor([2])\n",
      "328 tensor([0])\n",
      "329 tensor([2])\n",
      "330 tensor([1])\n",
      "331 tensor([2])\n",
      "332 tensor([1])\n",
      "333 tensor([0])\n",
      "334 tensor([1])\n",
      "335 tensor([1])\n",
      "336 tensor([1])\n",
      "337 tensor([0])\n",
      "338 tensor([2])\n",
      "339 tensor([0])\n",
      "340 tensor([1])\n",
      "341 tensor([2])\n",
      "342 tensor([1])\n",
      "343 tensor([0])\n",
      "344 tensor([0])\n",
      "345 tensor([2])\n",
      "346 tensor([1])\n",
      "347 tensor([0])\n",
      "348 tensor([1])\n",
      "349 tensor([2])\n",
      "350 tensor([1])\n",
      "351 tensor([2])\n",
      "352 tensor([1])\n",
      "353 tensor([1])\n",
      "354 tensor([1])\n",
      "355 tensor([0])\n",
      "356 tensor([2])\n",
      "357 tensor([0])\n",
      "358 tensor([1])\n",
      "359 tensor([0])\n",
      "360 tensor([1])\n",
      "361 tensor([0])\n",
      "362 tensor([0])\n",
      "363 tensor([1])\n",
      "364 tensor([2])\n",
      "365 tensor([0])\n",
      "366 tensor([1])\n",
      "367 tensor([1])\n",
      "368 tensor([1])\n",
      "369 tensor([2])\n",
      "370 tensor([1])\n",
      "371 tensor([0])\n",
      "372 tensor([0])\n",
      "373 tensor([1])\n",
      "374 tensor([0])\n",
      "375 tensor([0])\n",
      "376 tensor([0])\n",
      "377 tensor([2])\n",
      "378 tensor([0])\n",
      "379 tensor([2])\n",
      "380 tensor([1])\n",
      "381 tensor([0])\n",
      "382 tensor([1])\n",
      "383 tensor([2])\n",
      "384 tensor([0])\n",
      "385 tensor([0])\n",
      "386 tensor([0])\n",
      "387 tensor([1])\n",
      "388 tensor([1])\n",
      "389 tensor([2])\n",
      "390 tensor([1])\n",
      "391 tensor([0])\n",
      "392 tensor([0])\n",
      "393 tensor([1])\n",
      "394 tensor([2])\n",
      "395 tensor([0])\n",
      "396 tensor([0])\n",
      "397 tensor([1])\n",
      "398 tensor([1])\n",
      "399 tensor([1])\n",
      "400 tensor([2])\n",
      "401 tensor([0])\n",
      "402 tensor([1])\n",
      "403 tensor([0])\n",
      "404 tensor([2])\n",
      "405 tensor([1])\n",
      "406 tensor([0])\n",
      "407 tensor([0])\n",
      "408 tensor([2])\n",
      "409 tensor([0])\n",
      "410 tensor([0])\n",
      "411 tensor([0])\n",
      "412 tensor([0])\n",
      "413 tensor([0])\n",
      "414 tensor([0])\n",
      "415 tensor([1])\n",
      "416 tensor([0])\n",
      "417 tensor([1])\n",
      "418 tensor([2])\n",
      "419 tensor([2])\n",
      "420 tensor([1])\n",
      "421 tensor([0])\n",
      "422 tensor([1])\n",
      "423 tensor([1])\n",
      "424 tensor([1])\n",
      "425 tensor([1])\n",
      "426 tensor([1])\n",
      "427 tensor([0])\n",
      "428 tensor([1])\n",
      "429 tensor([1])\n",
      "430 tensor([1])\n",
      "431 tensor([2])\n",
      "432 tensor([0])\n",
      "433 tensor([0])\n",
      "434 tensor([1])\n",
      "435 tensor([0])\n",
      "436 tensor([2])\n",
      "437 tensor([1])\n",
      "438 tensor([1])\n",
      "439 tensor([0])\n",
      "440 tensor([1])\n",
      "441 tensor([0])\n",
      "442 tensor([2])\n",
      "443 tensor([1])\n",
      "444 tensor([0])\n",
      "445 tensor([1])\n",
      "446 tensor([1])\n",
      "447 tensor([1])\n",
      "448 tensor([0])\n",
      "449 tensor([0])\n",
      "450 tensor([1])\n",
      "451 tensor([1])\n",
      "452 tensor([1])\n",
      "453 tensor([1])\n",
      "454 tensor([0])\n",
      "455 tensor([1])\n",
      "456 tensor([2])\n",
      "457 tensor([1])\n",
      "458 tensor([1])\n",
      "459 tensor([0])\n",
      "460 tensor([0])\n",
      "461 tensor([1])\n",
      "462 tensor([1])\n",
      "463 tensor([1])\n",
      "464 tensor([2])\n",
      "465 tensor([1])\n",
      "466 tensor([0])\n",
      "467 tensor([1])\n",
      "468 tensor([1])\n",
      "469 tensor([1])\n",
      "470 tensor([1])\n",
      "471 tensor([1])\n",
      "472 tensor([1])\n",
      "473 tensor([0])\n",
      "474 tensor([0])\n",
      "475 tensor([1])\n",
      "476 tensor([0])\n",
      "477 tensor([0])\n",
      "478 tensor([0])\n",
      "479 tensor([0])\n",
      "480 tensor([2])\n",
      "481 tensor([1])\n",
      "482 tensor([0])\n",
      "483 tensor([0])\n",
      "484 tensor([1])\n",
      "485 tensor([1])\n",
      "486 tensor([0])\n",
      "487 tensor([0])\n",
      "488 tensor([0])\n",
      "489 tensor([1])\n",
      "490 tensor([1])\n",
      "491 tensor([0])\n",
      "492 tensor([1])\n",
      "493 tensor([0])\n",
      "494 tensor([2])\n",
      "495 tensor([2])\n",
      "496 tensor([2])\n",
      "497 tensor([0])\n",
      "498 tensor([0])\n",
      "499 tensor([1])\n",
      "500 tensor([0])\n",
      "501 tensor([0])\n",
      "502 tensor([0])\n",
      "503 tensor([1])\n",
      "504 tensor([2])\n",
      "505 tensor([0])\n",
      "506 tensor([2])\n",
      "507 tensor([0])\n",
      "508 tensor([0])\n",
      "509 tensor([0])\n",
      "510 tensor([0])\n",
      "511 tensor([0])\n",
      "512 tensor([1])\n",
      "513 tensor([0])\n",
      "514 tensor([2])\n",
      "515 tensor([1])\n",
      "516 tensor([1])\n",
      "517 tensor([1])\n",
      "518 tensor([0])\n",
      "519 tensor([0])\n",
      "520 tensor([1])\n",
      "521 tensor([1])\n",
      "522 tensor([1])\n",
      "523 tensor([1])\n",
      "524 tensor([2])\n",
      "525 tensor([0])\n",
      "526 tensor([0])\n",
      "527 tensor([0])\n",
      "528 tensor([0])\n",
      "529 tensor([0])\n",
      "530 tensor([1])\n",
      "531 tensor([2])\n",
      "532 tensor([1])\n",
      "533 tensor([0])\n",
      "534 tensor([2])\n",
      "535 tensor([1])\n",
      "536 tensor([2])\n",
      "537 tensor([1])\n",
      "538 tensor([0])\n",
      "539 tensor([2])\n",
      "540 tensor([0])\n",
      "541 tensor([2])\n",
      "542 tensor([2])\n",
      "543 tensor([0])\n",
      "544 tensor([0])\n",
      "545 tensor([0])\n",
      "546 tensor([0])\n",
      "547 tensor([0])\n",
      "548 tensor([1])\n",
      "549 tensor([1])\n",
      "550 tensor([1])\n",
      "551 tensor([2])\n",
      "552 tensor([1])\n",
      "553 tensor([2])\n",
      "554 tensor([0])\n",
      "555 tensor([1])\n",
      "556 tensor([1])\n",
      "557 tensor([1])\n",
      "558 tensor([1])\n",
      "559 tensor([2])\n",
      "560 tensor([0])\n",
      "561 tensor([1])\n",
      "562 tensor([0])\n",
      "563 tensor([2])\n",
      "564 tensor([0])\n",
      "565 tensor([0])\n",
      "566 tensor([1])\n",
      "567 tensor([0])\n",
      "568 tensor([0])\n",
      "569 tensor([0])\n",
      "570 tensor([1])\n",
      "571 tensor([1])\n",
      "572 tensor([1])\n",
      "573 tensor([1])\n",
      "574 tensor([1])\n",
      "575 tensor([2])\n",
      "576 tensor([0])\n",
      "577 tensor([1])\n",
      "578 tensor([1])\n",
      "579 tensor([0])\n",
      "580 tensor([0])\n",
      "581 tensor([0])\n",
      "582 tensor([2])\n",
      "583 tensor([1])\n",
      "584 tensor([0])\n",
      "585 tensor([1])\n",
      "586 tensor([1])\n",
      "587 tensor([0])\n",
      "588 tensor([0])\n",
      "589 tensor([2])\n",
      "590 tensor([1])\n",
      "591 tensor([0])\n",
      "592 tensor([2])\n",
      "593 tensor([2])\n",
      "594 tensor([2])\n",
      "595 tensor([1])\n",
      "596 tensor([2])\n",
      "597 tensor([1])\n",
      "598 tensor([1])\n",
      "599 tensor([2])\n",
      "600 tensor([1])\n",
      "601 tensor([2])\n",
      "602 tensor([0])\n",
      "603 tensor([0])\n",
      "604 tensor([1])\n",
      "605 tensor([1])\n",
      "606 tensor([1])\n",
      "607 tensor([0])\n",
      "608 tensor([1])\n",
      "609 tensor([1])\n",
      "610 tensor([1])\n",
      "611 tensor([0])\n",
      "612 tensor([0])\n",
      "613 tensor([0])\n",
      "614 tensor([0])\n",
      "615 tensor([0])\n",
      "616 tensor([0])\n",
      "617 tensor([2])\n",
      "618 tensor([0])\n",
      "619 tensor([1])\n",
      "620 tensor([0])\n",
      "621 tensor([2])\n",
      "622 tensor([1])\n",
      "623 tensor([2])\n",
      "624 tensor([0])\n",
      "625 tensor([1])\n",
      "626 tensor([2])\n",
      "627 tensor([1])\n",
      "628 tensor([0])\n",
      "629 tensor([1])\n",
      "630 tensor([0])\n",
      "631 tensor([0])\n",
      "632 tensor([0])\n",
      "633 tensor([1])\n",
      "634 tensor([2])\n",
      "635 tensor([1])\n",
      "636 tensor([0])\n",
      "637 tensor([1])\n",
      "638 tensor([2])\n",
      "639 tensor([2])\n",
      "640 tensor([0])\n",
      "641 tensor([1])\n",
      "642 tensor([2])\n",
      "643 tensor([1])\n",
      "644 tensor([2])\n",
      "645 tensor([1])\n",
      "646 tensor([0])\n",
      "647 tensor([0])\n",
      "648 tensor([0])\n",
      "649 tensor([1])\n",
      "650 tensor([0])\n",
      "651 tensor([2])\n",
      "652 tensor([2])\n",
      "653 tensor([1])\n",
      "654 tensor([0])\n",
      "655 tensor([1])\n",
      "656 tensor([1])\n",
      "657 tensor([0])\n",
      "658 tensor([0])\n",
      "659 tensor([1])\n",
      "660 tensor([0])\n",
      "661 tensor([2])\n",
      "662 tensor([0])\n",
      "663 tensor([1])\n",
      "664 tensor([1])\n",
      "665 tensor([0])\n",
      "666 tensor([0])\n",
      "667 tensor([0])\n",
      "668 tensor([2])\n",
      "669 tensor([0])\n",
      "670 tensor([0])\n",
      "671 tensor([0])\n",
      "672 tensor([2])\n",
      "673 tensor([0])\n",
      "674 tensor([1])\n",
      "675 tensor([0])\n",
      "676 tensor([2])\n",
      "677 tensor([1])\n",
      "678 tensor([2])\n",
      "679 tensor([2])\n",
      "680 tensor([1])\n",
      "681 tensor([0])\n",
      "682 tensor([1])\n",
      "683 tensor([1])\n",
      "684 tensor([2])\n",
      "685 tensor([0])\n",
      "686 tensor([1])\n",
      "687 tensor([2])\n",
      "688 tensor([0])\n",
      "689 tensor([2])\n",
      "690 tensor([2])\n",
      "691 tensor([2])\n",
      "692 tensor([2])\n",
      "693 tensor([0])\n",
      "694 tensor([0])\n",
      "695 tensor([2])\n",
      "696 tensor([2])\n",
      "697 tensor([0])\n",
      "698 tensor([1])\n",
      "699 tensor([1])\n",
      "700 tensor([2])\n",
      "701 tensor([1])\n",
      "702 tensor([0])\n",
      "703 tensor([0])\n",
      "704 tensor([2])\n",
      "705 tensor([0])\n",
      "706 tensor([0])\n",
      "707 tensor([0])\n",
      "708 tensor([2])\n",
      "709 tensor([1])\n",
      "710 tensor([1])\n",
      "711 tensor([1])\n",
      "712 tensor([1])\n",
      "713 tensor([1])\n",
      "714 tensor([1])\n",
      "715 tensor([1])\n",
      "716 tensor([1])\n",
      "717 tensor([1])\n",
      "718 tensor([1])\n",
      "719 tensor([1])\n",
      "720 tensor([1])\n",
      "721 tensor([1])\n",
      "722 tensor([1])\n",
      "723 tensor([0])\n",
      "724 tensor([0])\n",
      "725 tensor([1])\n",
      "726 tensor([1])\n",
      "727 tensor([1])\n",
      "728 tensor([0])\n",
      "729 tensor([0])\n",
      "730 tensor([2])\n",
      "731 tensor([1])\n",
      "732 tensor([0])\n",
      "733 tensor([0])\n",
      "734 tensor([2])\n",
      "735 tensor([1])\n",
      "736 tensor([2])\n",
      "737 tensor([1])\n",
      "738 tensor([2])\n",
      "739 tensor([2])\n",
      "740 tensor([1])\n",
      "741 tensor([0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742 tensor([1])\n",
      "743 tensor([1])\n",
      "744 tensor([1])\n",
      "745 tensor([2])\n",
      "746 tensor([1])\n",
      "747 tensor([1])\n",
      "748 tensor([1])\n",
      "749 tensor([2])\n",
      "750 tensor([2])\n",
      "751 tensor([2])\n",
      "752 tensor([2])\n",
      "753 tensor([1])\n",
      "754 tensor([1])\n",
      "755 tensor([0])\n",
      "756 tensor([2])\n",
      "757 tensor([1])\n",
      "758 tensor([0])\n",
      "759 tensor([2])\n",
      "760 tensor([0])\n",
      "761 tensor([1])\n",
      "762 tensor([0])\n",
      "763 tensor([1])\n",
      "764 tensor([0])\n",
      "765 tensor([0])\n",
      "766 tensor([1])\n",
      "767 tensor([0])\n",
      "768 tensor([1])\n",
      "769 tensor([1])\n",
      "770 tensor([2])\n",
      "771 tensor([1])\n",
      "772 tensor([0])\n",
      "773 tensor([1])\n",
      "774 tensor([1])\n",
      "775 tensor([2])\n",
      "776 tensor([2])\n",
      "777 tensor([2])\n",
      "778 tensor([0])\n",
      "779 tensor([2])\n",
      "780 tensor([0])\n",
      "781 tensor([0])\n",
      "782 tensor([2])\n",
      "783 tensor([1])\n",
      "784 tensor([0])\n",
      "785 tensor([1])\n",
      "786 tensor([1])\n",
      "787 tensor([0])\n",
      "788 tensor([1])\n",
      "789 tensor([0])\n",
      "790 tensor([0])\n",
      "791 tensor([2])\n",
      "792 tensor([2])\n",
      "793 tensor([0])\n",
      "794 tensor([1])\n",
      "795 tensor([0])\n",
      "796 tensor([0])\n",
      "797 tensor([0])\n",
      "798 tensor([0])\n",
      "799 tensor([0])\n",
      "800 tensor([1])\n",
      "801 tensor([0])\n",
      "802 tensor([2])\n",
      "803 tensor([2])\n",
      "804 tensor([0])\n",
      "805 tensor([0])\n",
      "806 tensor([1])\n",
      "807 tensor([1])\n",
      "808 tensor([1])\n",
      "809 tensor([1])\n",
      "810 tensor([0])\n",
      "811 tensor([2])\n",
      "812 tensor([0])\n",
      "813 tensor([1])\n",
      "814 tensor([0])\n",
      "815 tensor([2])\n",
      "816 tensor([2])\n",
      "817 tensor([1])\n",
      "818 tensor([0])\n",
      "819 tensor([1])\n",
      "820 tensor([0])\n",
      "821 tensor([1])\n",
      "822 tensor([0])\n",
      "823 tensor([0])\n",
      "824 tensor([1])\n",
      "825 tensor([0])\n",
      "826 tensor([2])\n",
      "827 tensor([0])\n",
      "828 tensor([1])\n",
      "829 tensor([1])\n",
      "830 tensor([2])\n",
      "831 tensor([0])\n",
      "832 tensor([0])\n",
      "833 tensor([0])\n",
      "834 tensor([2])\n",
      "835 tensor([2])\n",
      "836 tensor([1])\n",
      "837 tensor([1])\n",
      "838 tensor([1])\n",
      "839 tensor([0])\n",
      "840 tensor([1])\n",
      "841 tensor([1])\n",
      "842 tensor([1])\n",
      "843 tensor([1])\n",
      "844 tensor([1])\n",
      "845 tensor([1])\n",
      "846 tensor([1])\n",
      "847 tensor([1])\n",
      "848 tensor([1])\n",
      "849 tensor([1])\n",
      "850 tensor([1])\n",
      "851 tensor([2])\n",
      "852 tensor([1])\n",
      "853 tensor([0])\n",
      "854 tensor([0])\n",
      "855 tensor([1])\n",
      "856 tensor([0])\n",
      "857 tensor([2])\n",
      "858 tensor([2])\n",
      "859 tensor([2])\n",
      "860 tensor([1])\n",
      "861 tensor([2])\n",
      "862 tensor([0])\n",
      "863 tensor([2])\n",
      "864 tensor([1])\n",
      "865 tensor([1])\n",
      "866 tensor([1])\n",
      "867 tensor([1])\n",
      "868 tensor([0])\n",
      "869 tensor([1])\n",
      "870 tensor([1])\n",
      "871 tensor([2])\n",
      "872 tensor([2])\n",
      "873 tensor([0])\n",
      "874 tensor([1])\n",
      "875 tensor([0])\n",
      "876 tensor([2])\n",
      "877 tensor([1])\n",
      "878 tensor([1])\n",
      "879 tensor([0])\n",
      "880 tensor([1])\n",
      "881 tensor([2])\n",
      "882 tensor([0])\n",
      "883 tensor([1])\n",
      "884 tensor([0])\n",
      "885 tensor([0])\n",
      "886 tensor([2])\n",
      "887 tensor([0])\n",
      "888 tensor([1])\n",
      "889 tensor([2])\n",
      "890 tensor([2])\n",
      "891 tensor([1])\n",
      "892 tensor([1])\n",
      "893 tensor([0])\n",
      "894 tensor([1])\n",
      "895 tensor([1])\n",
      "896 tensor([0])\n",
      "897 tensor([1])\n",
      "898 tensor([2])\n",
      "899 tensor([0])\n",
      "900 tensor([1])\n",
      "901 tensor([1])\n",
      "902 tensor([1])\n",
      "903 tensor([2])\n",
      "904 tensor([0])\n",
      "905 tensor([0])\n",
      "906 tensor([0])\n",
      "907 tensor([1])\n",
      "908 tensor([1])\n",
      "909 tensor([2])\n",
      "910 tensor([1])\n",
      "911 tensor([0])\n",
      "912 tensor([0])\n",
      "913 tensor([2])\n",
      "914 tensor([1])\n",
      "915 tensor([1])\n",
      "916 tensor([1])\n",
      "917 tensor([1])\n",
      "918 tensor([1])\n",
      "919 tensor([1])\n",
      "920 tensor([1])\n",
      "921 tensor([0])\n",
      "922 tensor([0])\n",
      "923 tensor([0])\n",
      "924 tensor([1])\n",
      "925 tensor([1])\n",
      "926 tensor([0])\n",
      "927 tensor([0])\n",
      "928 tensor([1])\n",
      "929 tensor([0])\n",
      "930 tensor([0])\n",
      "931 tensor([0])\n",
      "932 tensor([2])\n",
      "933 tensor([0])\n",
      "934 tensor([2])\n",
      "935 tensor([2])\n",
      "936 tensor([0])\n",
      "937 tensor([1])\n",
      "938 tensor([1])\n",
      "939 tensor([1])\n",
      "940 tensor([0])\n",
      "941 tensor([0])\n",
      "942 tensor([2])\n",
      "943 tensor([2])\n",
      "944 tensor([1])\n",
      "945 tensor([2])\n",
      "946 tensor([0])\n",
      "947 tensor([1])\n",
      "948 tensor([0])\n",
      "949 tensor([0])\n",
      "950 tensor([2])\n",
      "951 tensor([2])\n",
      "952 tensor([1])\n",
      "953 tensor([0])\n",
      "954 tensor([1])\n",
      "955 tensor([1])\n",
      "956 tensor([1])\n",
      "957 tensor([1])\n",
      "958 tensor([1])\n",
      "959 tensor([0])\n",
      "960 tensor([2])\n",
      "961 tensor([2])\n",
      "962 tensor([2])\n",
      "963 tensor([2])\n",
      "964 tensor([1])\n",
      "965 tensor([0])\n",
      "966 tensor([2])\n",
      "967 tensor([1])\n",
      "968 tensor([0])\n",
      "969 tensor([1])\n",
      "970 tensor([0])\n",
      "971 tensor([1])\n",
      "972 tensor([1])\n",
      "973 tensor([0])\n",
      "974 tensor([2])\n",
      "975 tensor([0])\n",
      "976 tensor([0])\n",
      "977 tensor([1])\n",
      "978 tensor([0])\n",
      "979 tensor([0])\n",
      "980 tensor([1])\n",
      "981 tensor([0])\n",
      "982 tensor([0])\n",
      "983 tensor([0])\n",
      "984 tensor([1])\n",
      "985 tensor([0])\n",
      "986 tensor([0])\n",
      "987 tensor([1])\n",
      "988 tensor([0])\n",
      "989 tensor([1])\n",
      "990 tensor([0])\n",
      "991 tensor([1])\n",
      "992 tensor([1])\n",
      "993 tensor([2])\n",
      "994 tensor([1])\n",
      "995 tensor([0])\n",
      "996 tensor([1])\n",
      "997 tensor([1])\n",
      "998 tensor([0])\n",
      "999 tensor([1])\n",
      "1000 tensor([2])\n",
      "1001 tensor([1])\n",
      "1002 tensor([0])\n",
      "1003 tensor([2])\n",
      "1004 tensor([0])\n",
      "1005 tensor([0])\n",
      "1006 tensor([0])\n",
      "1007 tensor([0])\n",
      "1008 tensor([1])\n",
      "1009 tensor([1])\n",
      "1010 tensor([0])\n",
      "1011 tensor([1])\n",
      "1012 tensor([0])\n",
      "1013 tensor([1])\n",
      "1014 tensor([1])\n",
      "1015 tensor([1])\n",
      "1016 tensor([1])\n",
      "1017 tensor([2])\n",
      "1018 tensor([0])\n",
      "1019 tensor([0])\n",
      "1020 tensor([0])\n",
      "1021 tensor([2])\n",
      "1022 tensor([1])\n",
      "1023 tensor([1])\n",
      "1024 tensor([1])\n",
      "1025 tensor([0])\n",
      "1026 tensor([1])\n",
      "1027 tensor([2])\n",
      "1028 tensor([0])\n",
      "1029 tensor([1])\n",
      "1030 tensor([0])\n",
      "1031 tensor([0])\n",
      "1032 tensor([0])\n",
      "1033 tensor([1])\n",
      "1034 tensor([1])\n",
      "1035 tensor([2])\n",
      "1036 tensor([1])\n",
      "1037 tensor([1])\n",
      "1038 tensor([1])\n",
      "1039 tensor([1])\n",
      "1040 tensor([1])\n",
      "1041 tensor([1])\n",
      "1042 tensor([1])\n",
      "1043 tensor([1])\n",
      "1044 tensor([1])\n",
      "1045 tensor([1])\n",
      "1046 tensor([2])\n",
      "1047 tensor([0])\n",
      "1048 tensor([0])\n",
      "1049 tensor([0])\n",
      "1050 tensor([0])\n",
      "1051 tensor([0])\n",
      "1052 tensor([1])\n",
      "1053 tensor([1])\n",
      "1054 tensor([0])\n",
      "1055 tensor([2])\n",
      "1056 tensor([1])\n",
      "1057 tensor([1])\n",
      "1058 tensor([0])\n",
      "1059 tensor([0])\n",
      "1060 tensor([0])\n",
      "1061 tensor([0])\n",
      "1062 tensor([1])\n",
      "1063 tensor([2])\n",
      "1064 tensor([0])\n",
      "1065 tensor([0])\n",
      "1066 tensor([2])\n",
      "1067 tensor([1])\n",
      "1068 tensor([2])\n",
      "1069 tensor([1])\n",
      "1070 tensor([0])\n",
      "1071 tensor([1])\n",
      "1072 tensor([2])\n",
      "1073 tensor([0])\n",
      "1074 tensor([2])\n",
      "1075 tensor([0])\n",
      "1076 tensor([0])\n",
      "1077 tensor([1])\n",
      "1078 tensor([1])\n",
      "1079 tensor([2])\n",
      "1080 tensor([1])\n",
      "1081 tensor([2])\n",
      "1082 tensor([2])\n",
      "1083 tensor([2])\n",
      "1084 tensor([1])\n",
      "1085 tensor([0])\n",
      "1086 tensor([1])\n",
      "1087 tensor([2])\n",
      "1088 tensor([0])\n",
      "1089 tensor([0])\n",
      "1090 tensor([2])\n",
      "1091 tensor([2])\n",
      "1092 tensor([1])\n",
      "1093 tensor([1])\n",
      "1094 tensor([1])\n",
      "1095 tensor([1])\n",
      "1096 tensor([1])\n",
      "1097 tensor([2])\n",
      "1098 tensor([2])\n",
      "1099 tensor([2])\n",
      "1100 tensor([2])\n",
      "1101 tensor([2])\n",
      "1102 tensor([1])\n",
      "1103 tensor([1])\n",
      "1104 tensor([0])\n",
      "1105 tensor([0])\n",
      "1106 tensor([2])\n",
      "1107 tensor([2])\n",
      "1108 tensor([2])\n",
      "1109 tensor([1])\n",
      "1110 tensor([0])\n",
      "1111 tensor([0])\n",
      "1112 tensor([2])\n",
      "1113 tensor([2])\n",
      "1114 tensor([0])\n",
      "1115 tensor([2])\n",
      "1116 tensor([2])\n",
      "1117 tensor([1])\n",
      "1118 tensor([1])\n",
      "1119 tensor([2])\n",
      "1120 tensor([0])\n",
      "1121 tensor([1])\n",
      "1122 tensor([1])\n",
      "1123 tensor([1])\n",
      "1124 tensor([2])\n",
      "1125 tensor([1])\n",
      "1126 tensor([1])\n",
      "1127 tensor([1])\n",
      "1128 tensor([1])\n",
      "1129 tensor([2])\n",
      "1130 tensor([0])\n",
      "1131 tensor([1])\n",
      "1132 tensor([1])\n",
      "1133 tensor([1])\n",
      "1134 tensor([1])\n",
      "1135 tensor([2])\n",
      "1136 tensor([1])\n",
      "1137 tensor([1])\n",
      "1138 tensor([2])\n",
      "1139 tensor([1])\n",
      "1140 tensor([0])\n",
      "1141 tensor([0])\n",
      "1142 tensor([1])\n",
      "1143 tensor([1])\n",
      "1144 tensor([1])\n",
      "1145 tensor([0])\n",
      "1146 tensor([1])\n",
      "1147 tensor([1])\n",
      "1148 tensor([0])\n",
      "1149 tensor([0])\n",
      "1150 tensor([0])\n",
      "1151 tensor([1])\n",
      "1152 tensor([1])\n",
      "1153 tensor([0])\n",
      "1154 tensor([1])\n",
      "1155 tensor([2])\n",
      "1156 tensor([0])\n",
      "1157 tensor([0])\n",
      "1158 tensor([0])\n",
      "1159 tensor([1])\n",
      "1160 tensor([0])\n",
      "1161 tensor([1])\n",
      "1162 tensor([0])\n",
      "1163 tensor([1])\n",
      "1164 tensor([1])\n",
      "1165 tensor([1])\n",
      "1166 tensor([2])\n",
      "1167 tensor([1])\n",
      "1168 tensor([0])\n",
      "1169 tensor([2])\n",
      "1170 tensor([1])\n",
      "1171 tensor([1])\n",
      "1172 tensor([0])\n",
      "1173 tensor([1])\n",
      "1174 tensor([1])\n",
      "1175 tensor([2])\n",
      "1176 tensor([2])\n",
      "1177 tensor([0])\n",
      "1178 tensor([0])\n",
      "1179 tensor([1])\n",
      "1180 tensor([1])\n",
      "1181 tensor([0])\n",
      "1182 tensor([2])\n",
      "1183 tensor([0])\n",
      "1184 tensor([2])\n",
      "1185 tensor([1])\n",
      "1186 tensor([1])\n",
      "1187 tensor([2])\n",
      "1188 tensor([0])\n",
      "1189 tensor([0])\n",
      "1190 tensor([0])\n",
      "1191 tensor([1])\n",
      "1192 tensor([0])\n",
      "1193 tensor([0])\n",
      "1194 tensor([1])\n",
      "1195 tensor([1])\n",
      "1196 tensor([0])\n",
      "1197 tensor([1])\n",
      "1198 tensor([1])\n",
      "1199 tensor([0])\n",
      "1200 tensor([0])\n",
      "1201 tensor([0])\n",
      "1202 tensor([2])\n",
      "1203 tensor([1])\n",
      "1204 tensor([0])\n",
      "1205 tensor([0])\n",
      "1206 tensor([0])\n",
      "1207 tensor([0])\n",
      "1208 tensor([2])\n",
      "1209 tensor([1])\n",
      "1210 tensor([2])\n",
      "1211 tensor([2])\n",
      "1212 tensor([1])\n",
      "1213 tensor([2])\n",
      "1214 tensor([0])\n",
      "1215 tensor([0])\n",
      "1216 tensor([1])\n",
      "1217 tensor([1])\n",
      "1218 tensor([1])\n",
      "1219 tensor([0])\n",
      "1220 tensor([0])\n",
      "1221 tensor([0])\n",
      "1222 tensor([1])\n",
      "1223 tensor([0])\n",
      "1224 tensor([1])\n",
      "1225 tensor([0])\n",
      "1226 tensor([2])\n",
      "1227 tensor([1])\n",
      "1228 tensor([1])\n",
      "1229 tensor([0])\n",
      "1230 tensor([1])\n",
      "1231 tensor([2])\n",
      "1232 tensor([2])\n",
      "1233 tensor([1])\n",
      "1234 tensor([2])\n",
      "1235 tensor([2])\n",
      "1236 tensor([0])\n",
      "1237 tensor([2])\n",
      "1238 tensor([1])\n",
      "1239 tensor([1])\n",
      "1240 tensor([0])\n",
      "1241 tensor([1])\n",
      "1242 tensor([0])\n",
      "1243 tensor([2])\n",
      "1244 tensor([0])\n",
      "1245 tensor([0])\n",
      "1246 tensor([0])\n",
      "1247 tensor([1])\n",
      "1248 tensor([1])\n",
      "1249 tensor([0])\n",
      "1250 tensor([2])\n",
      "1251 tensor([0])\n",
      "1252 tensor([2])\n",
      "1253 tensor([1])\n",
      "1254 tensor([2])\n",
      "1255 tensor([2])\n",
      "1256 tensor([0])\n",
      "1257 tensor([0])\n",
      "1258 tensor([1])\n",
      "1259 tensor([1])\n",
      "1260 tensor([1])\n",
      "1261 tensor([1])\n",
      "1262 tensor([1])\n",
      "1263 tensor([0])\n",
      "1264 tensor([1])\n",
      "1265 tensor([0])\n",
      "1266 tensor([1])\n",
      "1267 tensor([0])\n",
      "1268 tensor([1])\n",
      "1269 tensor([2])\n",
      "1270 tensor([1])\n",
      "1271 tensor([2])\n",
      "1272 tensor([1])\n",
      "1273 tensor([1])\n",
      "1274 tensor([1])\n",
      "1275 tensor([1])\n",
      "1276 tensor([1])\n",
      "1277 tensor([0])\n",
      "1278 tensor([0])\n",
      "1279 tensor([0])\n",
      "1280 tensor([1])\n",
      "1281 tensor([0])\n",
      "1282 tensor([0])\n",
      "1283 tensor([1])\n",
      "1284 tensor([2])\n",
      "1285 tensor([2])\n",
      "1286 tensor([0])\n",
      "1287 tensor([0])\n",
      "1288 tensor([2])\n",
      "1289 tensor([2])\n",
      "1290 tensor([1])\n",
      "1291 tensor([0])\n",
      "1292 tensor([2])\n",
      "1293 tensor([1])\n",
      "1294 tensor([1])\n",
      "1295 tensor([1])\n",
      "1296 tensor([0])\n",
      "1297 tensor([1])\n",
      "1298 tensor([1])\n",
      "1299 tensor([1])\n",
      "1300 tensor([0])\n",
      "1301 tensor([1])\n",
      "1302 tensor([0])\n",
      "1303 tensor([1])\n",
      "1304 tensor([2])\n",
      "1305 tensor([2])\n",
      "1306 tensor([0])\n",
      "1307 tensor([2])\n",
      "1308 tensor([2])\n",
      "1309 tensor([1])\n",
      "1310 tensor([1])\n",
      "1311 tensor([1])\n",
      "1312 tensor([0])\n",
      "1313 tensor([2])\n",
      "1314 tensor([1])\n",
      "1315 tensor([1])\n",
      "1316 tensor([1])\n",
      "1317 tensor([1])\n",
      "1318 tensor([2])\n",
      "1319 tensor([0])\n",
      "1320 tensor([0])\n",
      "1321 tensor([1])\n",
      "1322 tensor([1])\n",
      "1323 tensor([1])\n",
      "1324 tensor([0])\n",
      "1325 tensor([1])\n",
      "1326 tensor([0])\n",
      "1327 tensor([2])\n",
      "1328 tensor([2])\n",
      "1329 tensor([0])\n",
      "1330 tensor([0])\n",
      "1331 tensor([1])\n",
      "1332 tensor([2])\n",
      "1333 tensor([1])\n",
      "1334 tensor([1])\n",
      "1335 tensor([1])\n",
      "1336 tensor([0])\n",
      "1337 tensor([1])\n",
      "1338 tensor([1])\n",
      "1339 tensor([1])\n",
      "1340 tensor([0])\n",
      "1341 tensor([1])\n",
      "1342 tensor([0])\n",
      "1343 tensor([1])\n",
      "1344 tensor([1])\n",
      "1345 tensor([0])\n",
      "1346 tensor([1])\n",
      "1347 tensor([0])\n",
      "1348 tensor([2])\n",
      "1349 tensor([1])\n",
      "1350 tensor([2])\n",
      "1351 tensor([1])\n",
      "1352 tensor([2])\n",
      "1353 tensor([1])\n",
      "1354 tensor([0])\n",
      "1355 tensor([1])\n",
      "1356 tensor([0])\n",
      "1357 tensor([1])\n",
      "1358 tensor([0])\n",
      "1359 tensor([2])\n",
      "1360 tensor([2])\n",
      "1361 tensor([0])\n",
      "1362 tensor([0])\n",
      "1363 tensor([0])\n",
      "1364 tensor([2])\n",
      "1365 tensor([1])\n",
      "1366 tensor([1])\n",
      "1367 tensor([1])\n",
      "1368 tensor([2])\n",
      "1369 tensor([1])\n",
      "1370 tensor([2])\n",
      "1371 tensor([2])\n",
      "1372 tensor([0])\n",
      "1373 tensor([0])\n",
      "1374 tensor([1])\n",
      "1375 tensor([2])\n",
      "1376 tensor([1])\n",
      "1377 tensor([2])\n",
      "1378 tensor([1])\n",
      "1379 tensor([2])\n",
      "1380 tensor([1])\n",
      "1381 tensor([2])\n",
      "1382 tensor([1])\n",
      "1383 tensor([0])\n",
      "1384 tensor([2])\n",
      "1385 tensor([0])\n",
      "1386 tensor([0])\n",
      "1387 tensor([2])\n",
      "1388 tensor([2])\n",
      "1389 tensor([2])\n",
      "1390 tensor([0])\n",
      "1391 tensor([1])\n",
      "1392 tensor([1])\n",
      "1393 tensor([1])\n",
      "1394 tensor([1])\n",
      "1395 tensor([1])\n",
      "1396 tensor([0])\n",
      "1397 tensor([0])\n",
      "1398 tensor([0])\n",
      "1399 tensor([1])\n",
      "1400 tensor([2])\n",
      "1401 tensor([0])\n",
      "1402 tensor([1])\n",
      "1403 tensor([2])\n",
      "1404 tensor([1])\n",
      "1405 tensor([1])\n",
      "1406 tensor([1])\n",
      "1407 tensor([0])\n",
      "1408 tensor([1])\n",
      "1409 tensor([1])\n",
      "1410 tensor([1])\n",
      "1411 tensor([0])\n",
      "1412 tensor([0])\n",
      "1413 tensor([0])\n",
      "1414 tensor([1])\n",
      "1415 tensor([2])\n",
      "1416 tensor([1])\n",
      "1417 tensor([2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418 tensor([2])\n",
      "1419 tensor([0])\n",
      "1420 tensor([2])\n",
      "1421 tensor([0])\n",
      "1422 tensor([1])\n",
      "1423 tensor([0])\n",
      "1424 tensor([0])\n",
      "1425 tensor([1])\n",
      "1426 tensor([0])\n",
      "1427 tensor([0])\n",
      "1428 tensor([2])\n",
      "1429 tensor([0])\n",
      "1430 tensor([0])\n",
      "1431 tensor([2])\n",
      "1432 tensor([2])\n",
      "1433 tensor([1])\n",
      "1434 tensor([2])\n",
      "1435 tensor([1])\n",
      "1436 tensor([1])\n",
      "1437 tensor([1])\n",
      "1438 tensor([0])\n",
      "1439 tensor([0])\n",
      "1440 tensor([0])\n",
      "1441 tensor([1])\n",
      "1442 tensor([1])\n",
      "1443 tensor([0])\n",
      "1444 tensor([1])\n",
      "1445 tensor([1])\n",
      "1446 tensor([2])\n",
      "1447 tensor([1])\n",
      "1448 tensor([0])\n",
      "1449 tensor([1])\n",
      "1450 tensor([0])\n",
      "1451 tensor([2])\n",
      "1452 tensor([2])\n",
      "1453 tensor([0])\n",
      "1454 tensor([1])\n",
      "1455 tensor([0])\n",
      "1456 tensor([1])\n",
      "1457 tensor([1])\n",
      "1458 tensor([1])\n",
      "1459 tensor([1])\n",
      "1460 tensor([0])\n",
      "1461 tensor([2])\n",
      "1462 tensor([0])\n",
      "1463 tensor([0])\n",
      "1464 tensor([1])\n",
      "1465 tensor([0])\n",
      "1466 tensor([1])\n",
      "1467 tensor([1])\n",
      "1468 tensor([0])\n",
      "1469 tensor([1])\n",
      "1470 tensor([0])\n",
      "1471 tensor([2])\n",
      "1472 tensor([1])\n",
      "1473 tensor([2])\n",
      "1474 tensor([2])\n",
      "1475 tensor([0])\n",
      "1476 tensor([2])\n",
      "1477 tensor([0])\n",
      "1478 tensor([1])\n",
      "1479 tensor([1])\n",
      "1480 tensor([1])\n",
      "1481 tensor([2])\n",
      "1482 tensor([2])\n",
      "1483 tensor([0])\n",
      "1484 tensor([1])\n",
      "1485 tensor([0])\n",
      "1486 tensor([2])\n",
      "1487 tensor([0])\n",
      "1488 tensor([1])\n",
      "1489 tensor([0])\n",
      "1490 tensor([1])\n",
      "1491 tensor([0])\n",
      "1492 tensor([0])\n",
      "1493 tensor([0])\n",
      "1494 tensor([2])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    input_ids, labels, masks = batch\n",
    "    print(step,labels)\n",
    "#     outputs = model(\n",
    "#         input_ids,\n",
    "#         attention_mask=masks,\n",
    "#         labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  102,  1996,  3857,  6279,  1997,  2522,  2475,  3310,  2013,  5255,\n",
       "          10725, 20145,  1010,  2004,  2092,  2004,  2013,  2455,  1011,  2224,\n",
       "           3431,  1012,   102,   101,  2122,  2047,  3463,  2097,  2393,  2149,\n",
       "           3437,  2590,  3980,  1999,  3408,  1997,  2119,  2712,  1011,  2504,\n",
       "           4125,  1998,  2129,  1996,  4774,  1005,  1055,  3147,  4655,  2024,\n",
       "          14120,  2000,  3795,  2689,  1012,   102, 17512,  3137, 21193,  2031,\n",
       "           2468,  2426,  1996,  2087,  4069, 18407,  1997,  3795, 12959,  1012,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([1]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids,labels,masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6573, grad_fn=<MultiMarginLossBackward>),\n",
       " tensor([[-0.3640, -0.1409,  0.0541]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,logits = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\n",
      "[SEP] the build ##up of co ##2 comes from burning fossil fuels , as well as from land - use changes . [SEP] [CLS] these new results will help us answer important questions in terms of both sea - level rise and how the planet ' s cold regions are responding to global change . [SEP] retreating mountain glaciers have become among the most prominent icons of global warming . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "label: tensor(1)\n",
      "\n",
      "pred: 2\n",
      "\n",
      "dist: [0.26532257 0.3316397  0.4030377 ]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#labels = labels.cpu().numpy()\n",
    "#input_ids = input_ids.cpu().numpy()\n",
    "preds = scipy.special.softmax(logits.detach().numpy(), axis=1)\n",
    "input_toks = [\n",
    "    tokenizer.convert_ids_to_tokens(s) for s in input_ids\n",
    "]\n",
    "\n",
    "for seq, label, pred in zip(input_toks, labels, preds):\n",
    "    sep_char = '+' if np.argmax(pred) == label else '-'\n",
    "    print(sep_char * 40 + '\\n')\n",
    "    print(' '.join(seq) + '\\n')\n",
    "    print('label: ' + str(label) + '\\n')\n",
    "    print('pred: ' + str(np.argmax(pred)) + '\\n')\n",
    "    print('dist: ' + str(pred) + '\\n')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_i in range(0, ARGS.num_epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, ARGS.num_epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            #print(step,batch)\n",
    "\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Loss: {:.2f}'.format(\n",
    "                    step, len(train_dataloader), elapsed, float(np.mean(losses))))\n",
    "\n",
    "            if CUDA:\n",
    "                batch = (x.cuda() for x in batch)\n",
    "            input_ids, labels, masks = batch\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                attention_mask=masks,\n",
    "                labels=labels)\n",
    "\n",
    "            #print(len(outputs))\n",
    "\n",
    "            #loss, _, _ = outputs\n",
    "            loss, _ = outputs\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        #writer.add_scalar('train/loss', np.mean(avg_loss), epoch_i)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "# ========================================\n",
    "#               Validation\n",
    "# ========================================\n",
    "print(\"\")\n",
    "print(\"Running Validation...\")\n",
    "\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "losses = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "log = open(ARGS.output_dir + '/log', 'w')\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "    if CUDA:\n",
    "        batch = (x.cuda() for x in batch)\n",
    "    input_ids, labels, masks = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=masks,\n",
    "            labels=labels)\n",
    "    #loss, logits, attns = outputs\n",
    "    loss, logits = outputs\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    labels = labels.cpu().numpy()\n",
    "    input_ids = input_ids.cpu().numpy()\n",
    "    preds = scipy.special.softmax(logits.cpu().numpy(), axis=1)\n",
    "    input_toks = [\n",
    "        tokenizer.convert_ids_to_tokens(s) for s in input_ids\n",
    "    ]\n",
    "\n",
    "    for seq, label, pred in zip(input_toks, labels, preds):\n",
    "        sep_char = '+' if np.argmax(pred) == label else '-'\n",
    "        log.write(sep_char * 40 + '\\n')\n",
    "        log.write(' '.join(seq) + '\\n')\n",
    "        log.write('label: ' + str(label) + '\\n')\n",
    "        log.write('pred: ' + str(np.argmax(pred)) + '\\n')\n",
    "        log.write('dist: ' + str(pred) + '\\n')\n",
    "        log.write('\\n\\n')\n",
    "\n",
    "        all_preds += [pred]\n",
    "        all_labels += [label]\n",
    "log.close()\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "f1 = sklearn.metrics.f1_score(all_labels, np.argmax(all_preds, axis=1),average='macro')\n",
    "acc = sklearn.metrics.accuracy_score(all_labels, np.argmax(all_preds, axis=1))\n",
    "#auc = sklearn.metrics.roc_auc_score(all_labels, all_preds[:, 1])\n",
    "\n",
    "#writer.add_scalar('eval/acc', acc, epoch_i)\n",
    "#writer.add_scalar('eval/auc', auc, epoch_i)\n",
    "#writer.add_scalar('eval/f1', f1, epoch_i)\n",
    "#writer.add_scalar('eval/loss', f1, epoch_i)\n",
    "\n",
    "print(\"  Loss: {0:.2f}\".format(avg_loss))\n",
    "print(\"  Accuracy: {0:.2f}\".format(acc))\n",
    "print(\"  F1: {0:.2f}\".format(f1))\n",
    "#print(\"  AUC: {0:.2f}\".format(auc))\n",
    "print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "# Want to save: model, config, vocab, eval results, preds, training_args,\n",
    "result = {'acc': acc,\n",
    "    'f1':f1,\n",
    "    'cm':cm(np.argmax(all_preds, axis=1),all_labels)}\n",
    "\n",
    "preds_df = pd.DataFrame({'true':all_labels,\n",
    "'predicted':np.argmax(all_preds, axis=1)})\n",
    "preds_df.to_csv(ARGS.output_dir+'/{}.tsv'.format(ARGS.pred_file_name),sep='\\t',index=False)\n",
    "\n",
    "output_eval_file = os.path.join(ARGS.output_dir, \"eval_results_{}.txt\".format(ARGS.pred_file_name))\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
